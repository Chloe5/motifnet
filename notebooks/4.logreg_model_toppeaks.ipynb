{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import fxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data locations\n",
    "raw_data_path = '../data/peaks_data'\n",
    "outbase = '../output/peaks_data'\n",
    "train_out_path = '%s/texttrain' % outbase\n",
    "valid_out_path = '%s/textvalidate' % outbase\n",
    "test_out_path = '%s/texttest' % outbase\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 50\n",
    "num_batches_in_train = int(41305 / batch_size)\n",
    "num_epochs = 2\n",
    "capacity = 2000\n",
    "min_after_dequeue = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# retrieving models\n",
    "global_step_to_load = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are of size 1200\n",
      "outputs are of size 12\n",
      "../output/peaks_data/texttest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/pandas/core/series.py:2890: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# get training data\n",
    "(seq, label), info = fxns.get_seq_and_label(train_out_path)\n",
    "seq_batch, label_batch = tf.train.shuffle_batch([seq, label],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('inputs are of size', info['seq_len'])\n",
    "\n",
    "# get validation data\n",
    "(seqv, labelv), infov = fxns.get_seq_and_label(valid_out_path)\n",
    "seqv_batch, labelv_batch = tf.train.shuffle_batch([seqv, labelv],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('outputs are of size', info['label_len'])\n",
    "\n",
    "reload(fxns)\n",
    "print(test_out_path)\n",
    "(seqt, labelt), info = fxns.get_seq_and_label(test_out_path, num_epochs=None)\n",
    "seqt_batch, labelt_batch = tf.train.shuffle_batch([seqt, labelt],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get model -- logistic\n",
    "import logreg_model\n",
    "modelname='peaks_logreg'\n",
    "X, Y, loss, logits = logreg_model.get_logreg_model(info['seq_len'], info['label_len'])\n",
    "\n",
    "# get model -- generic convnet\n",
    "#import novel_model; reload(novel_model)\n",
    "#modelname='freeconv'\n",
    "#conv_infos = [(7,(1,20),(2,2),4),(5,(1,20),(2,2),7)]#, (8,(1,20),(2,2),5)]\n",
    "#conv_infos = [(14,(1,21),(2,2),2,4),(14,(1,8),(2,2),2,14),(7,(1,5),(2,2),2,14)]#, (8,(1,20),(2,2),5)]\n",
    "#X, Y, loss, logits = novel_model.get_novel_model(info['seq_len'], info['label_len'], conv_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,\n",
    "                                                 name='SGD-Optimizer')\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "# define other summaries we want\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 25 i 24 Avg loss in epoch(incomplete): 0.6082045650482177\n",
      "Epoch: 0 global_step 50 i 49 Avg loss in epoch(incomplete): 0.5409634828567504\n",
      "Epoch: 0 global_step 75 i 74 Avg loss in epoch(incomplete): 0.49670562823613484\n",
      "Epoch: 0 global_step 100 i 99 Avg loss in epoch(incomplete): 0.464265521466732\n",
      "Epoch: 0 global_step 125 i 124 Avg loss in epoch(incomplete): 0.43990754246711733\n",
      "Epoch: 0 global_step 150 i 149 Avg loss in epoch(incomplete): 0.42145836671193443\n",
      "Epoch: 0 global_step 175 i 174 Avg loss in epoch(incomplete): 0.4067633191176823\n",
      "Epoch: 0 global_step 200 i 199 Avg loss in epoch(incomplete): 0.3949027985334396\n",
      "Epoch: 0 global_step 225 i 224 Avg loss in epoch(incomplete): 0.3853319693936242\n",
      "saving\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.37707674074172975\n",
      "INFO:tensorflow:log/peaks_logreg-250 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.37707674074172975\n",
      "Epoch: 0 global_step 275 i 274 Avg loss in epoch(incomplete): 0.3702114580978047\n",
      "Epoch: 0 global_step 300 i 299 Avg loss in epoch(incomplete): 0.3642754399776459\n",
      "Epoch: 0 global_step 325 i 324 Avg loss in epoch(incomplete): 0.35927821902128365\n",
      "Epoch: 0 global_step 350 i 349 Avg loss in epoch(incomplete): 0.3546697155918394\n",
      "Epoch: 0 global_step 375 i 374 Avg loss in epoch(incomplete): 0.3507746980985006\n",
      "Epoch: 0 global_step 400 i 399 Avg loss in epoch(incomplete): 0.34727303616702554\n",
      "Epoch: 0 global_step 425 i 424 Avg loss in epoch(incomplete): 0.344079025703318\n",
      "Epoch: 0 global_step 450 i 449 Avg loss in epoch(incomplete): 0.3412095514933268\n",
      "Epoch: 0 global_step 475 i 474 Avg loss in epoch(incomplete): 0.33878962391301204\n",
      "saving\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.3365665621161461\n",
      "INFO:tensorflow:log/peaks_logreg-500 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.3365665621161461\n",
      "Epoch: 0 global_step 525 i 524 Avg loss in epoch(incomplete): 0.3345037153221312\n",
      "Epoch: 0 global_step 550 i 549 Avg loss in epoch(incomplete): 0.3325890290195292\n",
      "Epoch: 0 global_step 575 i 574 Avg loss in epoch(incomplete): 0.3307471540181533\n",
      "Epoch: 0 global_step 600 i 599 Avg loss in epoch(incomplete): 0.3290857646862666\n",
      "Epoch: 0 global_step 625 i 624 Avg loss in epoch(incomplete): 0.32752440605163574\n",
      "Epoch: 0 global_step 650 i 649 Avg loss in epoch(incomplete): 0.3260869040397497\n",
      "Epoch: 0 global_step 675 i 674 Avg loss in epoch(incomplete): 0.3246732082190337\n",
      "Epoch: 0 global_step 700 i 699 Avg loss in epoch(incomplete): 0.3234737893087523\n",
      "Epoch: 0 global_step 725 i 724 Avg loss in epoch(incomplete): 0.3222808040832651\n",
      "saving\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.32121361343065896\n",
      "INFO:tensorflow:log/peaks_logreg-750 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.32121361343065896\n",
      "Epoch: 0 global_step 775 i 774 Avg loss in epoch(incomplete): 0.3202081080405943\n",
      "Epoch: 0 global_step 800 i 799 Avg loss in epoch(incomplete): 0.3192795165255666\n",
      "Epoch: 0 global_step 825 i 824 Avg loss in epoch(incomplete): 0.3183863714969519\n",
      "saving\n",
      "Epoch: 0 global_step 826 i 825 Avg loss in epoch(incomplete): 0.31833696917384935\n",
      "INFO:tensorflow:log/peaks_logreg-826 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 850 i 23 Avg loss in epoch(incomplete): 0.2884834259748459\n",
      "Epoch: 1 global_step 875 i 48 Avg loss in epoch(incomplete): 0.2875052349908011\n",
      "Epoch: 1 global_step 900 i 73 Avg loss in epoch(incomplete): 0.28815775827781576\n",
      "Epoch: 1 global_step 925 i 98 Avg loss in epoch(incomplete): 0.28771726471005066\n",
      "Epoch: 1 global_step 950 i 123 Avg loss in epoch(incomplete): 0.2878500670194626\n",
      "Epoch: 1 global_step 975 i 148 Avg loss in epoch(incomplete): 0.2879118775361336\n",
      "saving\n",
      "Epoch: 1 global_step 1000 i 173 Avg loss in epoch(incomplete): 0.28772953940534046\n",
      "INFO:tensorflow:log/peaks_logreg-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 1000 i 173 Avg loss in epoch(incomplete): 0.28772953940534046\n",
      "Epoch: 1 global_step 1025 i 198 Avg loss in epoch(incomplete): 0.2876569454993435\n",
      "Epoch: 1 global_step 1050 i 223 Avg loss in epoch(incomplete): 0.28746412401752813\n",
      "Epoch: 1 global_step 1075 i 248 Avg loss in epoch(incomplete): 0.28738712606181105\n",
      "Epoch: 1 global_step 1100 i 273 Avg loss in epoch(incomplete): 0.28751042811539923\n",
      "Epoch: 1 global_step 1125 i 298 Avg loss in epoch(incomplete): 0.2872017523317433\n",
      "Epoch: 1 global_step 1150 i 323 Avg loss in epoch(incomplete): 0.28712963018520377\n",
      "Epoch: 1 global_step 1175 i 348 Avg loss in epoch(incomplete): 0.28703524765790706\n",
      "Epoch: 1 global_step 1200 i 373 Avg loss in epoch(incomplete): 0.2870188758653753\n",
      "Epoch: 1 global_step 1225 i 398 Avg loss in epoch(incomplete): 0.28704748737782165\n",
      "saving\n",
      "Epoch: 1 global_step 1250 i 423 Avg loss in epoch(incomplete): 0.2868572066555608\n",
      "INFO:tensorflow:log/peaks_logreg-1250 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 1250 i 423 Avg loss in epoch(incomplete): 0.2868572066555608\n",
      "Epoch: 1 global_step 1275 i 448 Avg loss in epoch(incomplete): 0.2868774999223467\n",
      "Epoch: 1 global_step 1300 i 473 Avg loss in epoch(incomplete): 0.28689892043041276\n",
      "Epoch: 1 global_step 1325 i 498 Avg loss in epoch(incomplete): 0.28685537768986996\n",
      "Epoch: 1 global_step 1350 i 523 Avg loss in epoch(incomplete): 0.28675928399080536\n",
      "Epoch: 1 global_step 1375 i 548 Avg loss in epoch(incomplete): 0.28673363931626356\n",
      "Epoch: 1 global_step 1400 i 573 Avg loss in epoch(incomplete): 0.2866883286719538\n",
      "Epoch: 1 global_step 1425 i 598 Avg loss in epoch(incomplete): 0.2866713897214708\n",
      "Epoch: 1 global_step 1450 i 623 Avg loss in epoch(incomplete): 0.28657266029562706\n",
      "Epoch: 1 global_step 1475 i 648 Avg loss in epoch(incomplete): 0.2865252855746147\n",
      "saving\n",
      "Epoch: 1 global_step 1500 i 673 Avg loss in epoch(incomplete): 0.28638688194115014\n",
      "INFO:tensorflow:log/peaks_logreg-1500 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 1500 i 673 Avg loss in epoch(incomplete): 0.28638688194115014\n",
      "Epoch: 1 global_step 1525 i 698 Avg loss in epoch(incomplete): 0.2863859483527182\n",
      "Epoch: 1 global_step 1550 i 723 Avg loss in epoch(incomplete): 0.2862390686052939\n",
      "Epoch: 1 global_step 1575 i 748 Avg loss in epoch(incomplete): 0.28627547319326924\n",
      "Epoch: 1 global_step 1600 i 773 Avg loss in epoch(incomplete): 0.28619026575593676\n",
      "Epoch: 1 global_step 1625 i 798 Avg loss in epoch(incomplete): 0.28608307380401743\n",
      "Epoch: 1 global_step 1650 i 823 Avg loss in epoch(incomplete): 0.28605560032488075\n",
      "saving\n",
      "Epoch: 1 global_step 1652 i 825 Avg loss in epoch(incomplete): 0.2860643955682727\n",
      "INFO:tensorflow:log/peaks_logreg-1652 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists('log/' + modelname):\n",
    "   tf.gfile.DeleteRecursively('log/' + modelname) \n",
    "\n",
    "# train\n",
    "\n",
    "print_every = 25\n",
    "save_every = 250\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        global_step = 0\n",
    "    else:\n",
    "        saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load))\n",
    "        global_step = global_step_to_load\n",
    "    \n",
    "#    print(layer)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('log/' + modelname + '/train', sess.graph)\n",
    "    writerv = tf.summary.FileWriter('log/' + modelname + '/valid')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(num_batches_in_train):\n",
    "            batch_X, batch_Y = sess.run([seq_batch, label_batch])\n",
    "            batch_Xv, batch_Yv = sess.run([seqv_batch, labelv_batch])\n",
    "            _, _loss = sess.run([update, loss],\n",
    "                            feed_dict={X: batch_X, Y: batch_Y})\n",
    "            total_loss += _loss #/ num_batches_in_train\n",
    "            \n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                    Y: batch_Y})\n",
    "            summaryv = sess.run(summary_op, feed_dict={X: batch_Xv,\n",
    "                                                    Y: batch_Yv})\n",
    "#            writer.add_summary(summary, global_step=epoch*num_batches_in_train + i)\n",
    "#            writerv.add_summary(summaryv, global_step=epoch*num_batches_in_train + i)\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "            writerv.add_summary(summaryv, global_step=global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % save_every == 0:\n",
    "                print('saving')\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "                saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "                \n",
    "            if global_step % print_every == 0:\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "        \n",
    "        print('saving')\n",
    "        print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "\n",
    "        saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "    \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/peaks_logreg-1652\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# get test data\n",
    "\n",
    "global_step_to_load_test = 1652\n",
    "\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load_test))\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < 4348 / batch_size:\n",
    "        batch_Xt, batch_Yt = sess.run([seqt_batch, labelt_batch])\n",
    "        batch_logits = sess.run(logits, feed_dict={X: batch_Xt, Y: batch_Yt})\n",
    "        scores.append(batch_logits)\n",
    "        labels.append(batch_Yt)\n",
    "        #print('asdf')\n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "scores_arr = np.concatenate(scores, axis=0)\n",
    "labels_arr = np.concatenate(labels, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:\n",
      "[0.89586207 0.9        0.89218391 0.90413793 0.90666667 0.9154023\n",
      " 0.92551724 0.9783908  0.90896552 0.89333333 0.97425287 0.90528736]\n",
      "aucs:\n",
      "[0.5502534637783861, 0.6189706551577341, 0.5376438812489817, 0.5986063932987858, 0.6152542616333097, 0.5111186480466448, 0.5162343685795416, 0.5898106302991521, 0.5856037614383593, 0.5887850984625178, 0.5294929380435516, 0.5031507788192715]\n",
      "auprcs:\n",
      "[0.12745621761966772, 0.13972939442568383, 0.1198177832454793, 0.12374583574894096, 0.1277459373211998, 0.0887785785052278, 0.07725619452143478, 0.034377931921971486, 0.12000200253785176, 0.1516961115527392, 0.030813398999815528, 0.10060326373600603]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>aucs</th>\n",
       "      <th>auprcs</th>\n",
       "      <th>props</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TBP</th>\n",
       "      <td>0.905287</td>\n",
       "      <td>0.503151</td>\n",
       "      <td>0.100603</td>\n",
       "      <td>0.094713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYC</th>\n",
       "      <td>0.915402</td>\n",
       "      <td>0.511119</td>\n",
       "      <td>0.088779</td>\n",
       "      <td>0.084598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFKB1</th>\n",
       "      <td>0.925517</td>\n",
       "      <td>0.516234</td>\n",
       "      <td>0.077256</td>\n",
       "      <td>0.074483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAF1</th>\n",
       "      <td>0.974253</td>\n",
       "      <td>0.529493</td>\n",
       "      <td>0.030813</td>\n",
       "      <td>0.025747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GATA2</th>\n",
       "      <td>0.892184</td>\n",
       "      <td>0.537644</td>\n",
       "      <td>0.119818</td>\n",
       "      <td>0.107586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTCF</th>\n",
       "      <td>0.895862</td>\n",
       "      <td>0.550253</td>\n",
       "      <td>0.127456</td>\n",
       "      <td>0.104138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAX5</th>\n",
       "      <td>0.908966</td>\n",
       "      <td>0.585604</td>\n",
       "      <td>0.120002</td>\n",
       "      <td>0.091034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPI1</th>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.151696</td>\n",
       "      <td>0.106897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFKB2</th>\n",
       "      <td>0.978391</td>\n",
       "      <td>0.589811</td>\n",
       "      <td>0.034378</td>\n",
       "      <td>0.021609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JUN</th>\n",
       "      <td>0.904138</td>\n",
       "      <td>0.598606</td>\n",
       "      <td>0.123746</td>\n",
       "      <td>0.095862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAX</th>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.615254</td>\n",
       "      <td>0.127746</td>\n",
       "      <td>0.093333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOS</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.618971</td>\n",
       "      <td>0.139729</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = np.mean((scores_arr > 0).astype(int) == labels_arr.astype(int), axis=0)\n",
    "\n",
    "print('accuracies:')\n",
    "print(accuracies)\n",
    "\n",
    "import sklearn.metrics\n",
    "print('aucs:')\n",
    "aucs = [sklearn.metrics.roc_auc_score(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(aucs)\n",
    "\n",
    "import sklearn.metrics\n",
    "def prec_recall(ys_true, ys_hat):\n",
    "    ys, xs, thresholds = sklearn.metrics.precision_recall_curve(ys_true, ys_hat)\n",
    "    return sklearn.metrics.auc(xs, ys, reorder=True)\n",
    "\n",
    "print('auprcs:')\n",
    "auprcs = [prec_recall(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(auprcs)\n",
    "\n",
    "props = np.mean(labels_arr, axis=0)\n",
    "\n",
    "import json\n",
    "ids = sorted(json.loads(info['tf_to_pos'].replace(\"'\", '\"')).keys())\n",
    "\n",
    "from IPython.display import display_pretty, display_html\n",
    "import pandas as pd\n",
    "results = pd.DataFrame({'props':props, 'acc':accuracies, 'auprcs':auprcs, 'aucs':aucs}, index=ids).sort_values(by='aucs')\n",
    "display_html(results.to_html(), raw=True)\n",
    "results.to_csv('stats.' + modelname + '.tsv', index_label='id', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
