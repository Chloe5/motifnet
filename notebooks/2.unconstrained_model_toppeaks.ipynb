{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import fxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data locations\n",
    "raw_data_path = '../data/peaks_data'\n",
    "outbase = '../output/peaks_data'\n",
    "train_out_path = '%s/texttrain' % outbase\n",
    "valid_out_path = '%s/textvalidate' % outbase\n",
    "test_out_path = '%s/texttest' % outbase\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 50\n",
    "num_batches_in_train = int(41305 / batch_size)\n",
    "num_epochs = 2\n",
    "capacity = 2000\n",
    "min_after_dequeue = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# retrieving models\n",
    "global_step_to_load = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are of size 1200\n",
      "outputs are of size 12\n",
      "../output/peaks_data/texttest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/pandas/core/series.py:2890: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  infer_datetime_format=infer_datetime_format)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# get training data\n",
    "(seq, label), info = fxns.get_seq_and_label(train_out_path)\n",
    "seq_batch, label_batch = tf.train.shuffle_batch([seq, label],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('inputs are of size', info['seq_len'])\n",
    "\n",
    "# get validation data\n",
    "(seqv, labelv), infov = fxns.get_seq_and_label(valid_out_path)\n",
    "seqv_batch, labelv_batch = tf.train.shuffle_batch([seqv, labelv],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('outputs are of size', info['label_len'])\n",
    "\n",
    "reload(fxns)\n",
    "print(test_out_path)\n",
    "(seqt, labelt), info = fxns.get_seq_and_label(test_out_path, num_epochs=None)\n",
    "seqt_batch, labelt_batch = tf.train.shuffle_batch([seqt, labelt],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.0\n",
      "75.0\n",
      "38.0\n",
      "1200 h 4 38.0\n"
     ]
    }
   ],
   "source": [
    "# # get model -- logistic\n",
    "# import logreg_model\n",
    "# modelname='logreg'\n",
    "# X, Y, loss, logits = logreg_model.get_logreg_model(info['seq_len'], info['label_len'])\n",
    "\n",
    "# get model -- generic convnet\n",
    "import novel_model; reload(novel_model)\n",
    "modelname='peaks_freeconv'\n",
    "#conv_infos = [(7,(1,20),(2,2),4),(5,(1,20),(2,2),7)]#, (8,(1,20),(2,2),5)]\n",
    "conv_infos = [(14,(1,21),(2,2),2,4),(14,(1,8),(2,2),2,14),(7,(1,5),(2,2),2,14)]#, (8,(1,20),(2,2),5)]\n",
    "X, Y, loss, logits = novel_model.get_novel_model(info['seq_len'], info['label_len'], conv_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,\n",
    "                                                 name='SGD-Optimizer')\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "# define other summaries we want\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 25 i 24 Avg loss in epoch(incomplete): 0.7337293291091919\n",
      "Epoch: 0 global_step 50 i 49 Avg loss in epoch(incomplete): 0.6688433718681336\n",
      "Epoch: 0 global_step 75 i 74 Avg loss in epoch(incomplete): 0.60829052845637\n",
      "Epoch: 0 global_step 100 i 99 Avg loss in epoch(incomplete): 0.5480066582560539\n",
      "Epoch: 0 global_step 125 i 124 Avg loss in epoch(incomplete): 0.49955468916893003\n",
      "Epoch: 0 global_step 150 i 149 Avg loss in epoch(incomplete): 0.46433893382549285\n",
      "Epoch: 0 global_step 175 i 174 Avg loss in epoch(incomplete): 0.43836808596338545\n",
      "Epoch: 0 global_step 200 i 199 Avg loss in epoch(incomplete): 0.41879790619015694\n",
      "Epoch: 0 global_step 225 i 224 Avg loss in epoch(incomplete): 0.4034225683742099\n",
      "saving\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.39114264070987703\n",
      "INFO:tensorflow:log/peaks_freeconv-250 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.39114264070987703\n",
      "Epoch: 0 global_step 275 i 274 Avg loss in epoch(incomplete): 0.3810462654720653\n",
      "Epoch: 0 global_step 300 i 299 Avg loss in epoch(incomplete): 0.3725663442413012\n",
      "Epoch: 0 global_step 325 i 324 Avg loss in epoch(incomplete): 0.36554766664138205\n",
      "Epoch: 0 global_step 350 i 349 Avg loss in epoch(incomplete): 0.3593668569837298\n",
      "Epoch: 0 global_step 375 i 374 Avg loss in epoch(incomplete): 0.3541563369433085\n",
      "Epoch: 0 global_step 400 i 399 Avg loss in epoch(incomplete): 0.3494428324699402\n",
      "Epoch: 0 global_step 425 i 424 Avg loss in epoch(incomplete): 0.34535059031318216\n",
      "Epoch: 0 global_step 450 i 449 Avg loss in epoch(incomplete): 0.3417915806505415\n",
      "Epoch: 0 global_step 475 i 474 Avg loss in epoch(incomplete): 0.3385191393525977\n",
      "saving\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.33558661478757856\n",
      "INFO:tensorflow:log/peaks_freeconv-500 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.33558661478757856\n",
      "Epoch: 0 global_step 525 i 524 Avg loss in epoch(incomplete): 0.3328683974629357\n",
      "Epoch: 0 global_step 550 i 549 Avg loss in epoch(incomplete): 0.3304273893074556\n",
      "Epoch: 0 global_step 575 i 574 Avg loss in epoch(incomplete): 0.3282237455637559\n",
      "Epoch: 0 global_step 600 i 599 Avg loss in epoch(incomplete): 0.32618639965852103\n",
      "Epoch: 0 global_step 625 i 624 Avg loss in epoch(incomplete): 0.32430933237075804\n",
      "Epoch: 0 global_step 650 i 649 Avg loss in epoch(incomplete): 0.3225661227794794\n",
      "Epoch: 0 global_step 675 i 674 Avg loss in epoch(incomplete): 0.32093080304287097\n",
      "Epoch: 0 global_step 700 i 699 Avg loss in epoch(incomplete): 0.31950429814202447\n",
      "Epoch: 0 global_step 725 i 724 Avg loss in epoch(incomplete): 0.31811321291430245\n",
      "saving\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.31682074288527173\n",
      "INFO:tensorflow:log/peaks_freeconv-750 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.31682074288527173\n",
      "Epoch: 0 global_step 775 i 774 Avg loss in epoch(incomplete): 0.3156417244095956\n",
      "Epoch: 0 global_step 800 i 799 Avg loss in epoch(incomplete): 0.31449831984937193\n",
      "Epoch: 0 global_step 825 i 824 Avg loss in epoch(incomplete): 0.31339585278973436\n",
      "saving\n",
      "Epoch: 0 global_step 826 i 825 Avg loss in epoch(incomplete): 0.31335809040300494\n",
      "INFO:tensorflow:log/peaks_freeconv-826 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 850 i 23 Avg loss in epoch(incomplete): 0.2786274167398612\n",
      "Epoch: 1 global_step 875 i 48 Avg loss in epoch(incomplete): 0.2794923532982262\n",
      "Epoch: 1 global_step 900 i 73 Avg loss in epoch(incomplete): 0.2793852259983888\n",
      "Epoch: 1 global_step 925 i 98 Avg loss in epoch(incomplete): 0.2791818588068991\n",
      "Epoch: 1 global_step 950 i 123 Avg loss in epoch(incomplete): 0.2792987986918419\n",
      "Epoch: 1 global_step 975 i 148 Avg loss in epoch(incomplete): 0.2793662064027466\n",
      "saving\n",
      "Epoch: 1 global_step 1000 i 173 Avg loss in epoch(incomplete): 0.27920775321023217\n",
      "INFO:tensorflow:log/peaks_freeconv-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 1000 i 173 Avg loss in epoch(incomplete): 0.27920775321023217\n",
      "Epoch: 1 global_step 1025 i 198 Avg loss in epoch(incomplete): 0.27921825527545796\n",
      "Epoch: 1 global_step 1050 i 223 Avg loss in epoch(incomplete): 0.27933245271976503\n",
      "Epoch: 1 global_step 1075 i 248 Avg loss in epoch(incomplete): 0.27927101819390754\n",
      "Epoch: 1 global_step 1100 i 273 Avg loss in epoch(incomplete): 0.27937672455815504\n",
      "Epoch: 1 global_step 1125 i 298 Avg loss in epoch(incomplete): 0.2794290161053074\n",
      "Epoch: 1 global_step 1150 i 323 Avg loss in epoch(incomplete): 0.2794376643903462\n",
      "Epoch: 1 global_step 1175 i 348 Avg loss in epoch(incomplete): 0.27936687689797585\n",
      "Epoch: 1 global_step 1200 i 373 Avg loss in epoch(incomplete): 0.279380417045425\n",
      "Epoch: 1 global_step 1225 i 398 Avg loss in epoch(incomplete): 0.27929713313740895\n",
      "saving\n",
      "Epoch: 1 global_step 1250 i 423 Avg loss in epoch(incomplete): 0.27930685709107594\n",
      "INFO:tensorflow:log/peaks_freeconv-1250 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 1250 i 423 Avg loss in epoch(incomplete): 0.27930685709107594\n",
      "Epoch: 1 global_step 1275 i 448 Avg loss in epoch(incomplete): 0.2792732675118011\n",
      "Epoch: 1 global_step 1300 i 473 Avg loss in epoch(incomplete): 0.27923969301996354\n",
      "Epoch: 1 global_step 1325 i 498 Avg loss in epoch(incomplete): 0.27925786399889085\n",
      "Epoch: 1 global_step 1350 i 523 Avg loss in epoch(incomplete): 0.27923773240497096\n",
      "Epoch: 1 global_step 1375 i 548 Avg loss in epoch(incomplete): 0.27925129394930787\n",
      "Epoch: 1 global_step 1400 i 573 Avg loss in epoch(incomplete): 0.2793175613942462\n",
      "Epoch: 1 global_step 1425 i 598 Avg loss in epoch(incomplete): 0.27935321706165256\n",
      "Epoch: 1 global_step 1450 i 623 Avg loss in epoch(incomplete): 0.2793614703875322\n",
      "Epoch: 1 global_step 1475 i 648 Avg loss in epoch(incomplete): 0.279351899960008\n",
      "saving\n",
      "Epoch: 1 global_step 1500 i 673 Avg loss in epoch(incomplete): 0.2793704389376173\n",
      "INFO:tensorflow:log/peaks_freeconv-1500 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch: 1 global_step 1500 i 673 Avg loss in epoch(incomplete): 0.2793704389376173\n",
      "Epoch: 1 global_step 1525 i 698 Avg loss in epoch(incomplete): 0.27939196598205784\n",
      "Epoch: 1 global_step 1550 i 723 Avg loss in epoch(incomplete): 0.27938518603203705\n",
      "Epoch: 1 global_step 1575 i 748 Avg loss in epoch(incomplete): 0.27937685894234\n",
      "Epoch: 1 global_step 1600 i 773 Avg loss in epoch(incomplete): 0.27938902832432927\n",
      "Epoch: 1 global_step 1625 i 798 Avg loss in epoch(incomplete): 0.27937763302735\n",
      "Epoch: 1 global_step 1650 i 823 Avg loss in epoch(incomplete): 0.2793617719440784\n",
      "saving\n",
      "Epoch: 1 global_step 1652 i 825 Avg loss in epoch(incomplete): 0.2793540960362691\n",
      "INFO:tensorflow:log/peaks_freeconv-1652 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists('log/' + modelname):\n",
    "   tf.gfile.DeleteRecursively('log/' + modelname) \n",
    "\n",
    "# train\n",
    "\n",
    "print_every = 25\n",
    "save_every = 250\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        global_step = 0\n",
    "    else:\n",
    "        saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load))\n",
    "        global_step = global_step_to_load\n",
    "    \n",
    "#    print(layer)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('log/' + modelname + '/train', sess.graph)\n",
    "    writerv = tf.summary.FileWriter('log/' + modelname + '/valid')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(num_batches_in_train):\n",
    "            batch_X, batch_Y = sess.run([seq_batch, label_batch])\n",
    "            batch_Xv, batch_Yv = sess.run([seqv_batch, labelv_batch])\n",
    "            _, _loss = sess.run([update, loss],\n",
    "                            feed_dict={X: batch_X, Y: batch_Y})\n",
    "            total_loss += _loss #/ num_batches_in_train\n",
    "            \n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                    Y: batch_Y})\n",
    "            summaryv = sess.run(summary_op, feed_dict={X: batch_Xv,\n",
    "                                                    Y: batch_Yv})\n",
    "#            writer.add_summary(summary, global_step=epoch*num_batches_in_train + i)\n",
    "#            writerv.add_summary(summaryv, global_step=epoch*num_batches_in_train + i)\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "            writerv.add_summary(summaryv, global_step=global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % save_every == 0:\n",
    "                print('saving')\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "                saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "                \n",
    "            if global_step % print_every == 0:\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "        \n",
    "        print('saving')\n",
    "        print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "\n",
    "        saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "    \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/peaks_freeconv-1652\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# get test data\n",
    "\n",
    "global_step_to_load_test = 1652\n",
    "\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load_test))\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < 4348 / batch_size:\n",
    "        batch_Xt, batch_Yt = sess.run([seqt_batch, labelt_batch])\n",
    "        batch_logits = sess.run(logits, feed_dict={X: batch_Xt, Y: batch_Yt})\n",
    "        scores.append(batch_logits)\n",
    "        labels.append(batch_Yt)\n",
    "        #print('asdf')\n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "scores_arr = np.concatenate(scores, axis=0)\n",
    "labels_arr = np.concatenate(labels, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:\n",
      "[0.89172414 0.90390805 0.89586207 0.90413793 0.90804598 0.91425287\n",
      " 0.92643678 0.9783908  0.9062069  0.89448276 0.97494253 0.9016092 ]\n",
      "aucs:\n",
      "[0.5014162491810386, 0.4585547002389911, 0.5097870609700902, 0.4677719304342948, 0.49903132911392406, 0.4692241784361958, 0.5068300248138958, 0.5840315549512078, 0.48581235512977383, 0.5179529991841963, 0.5218541585094393, 0.5111955325101155]\n",
      "auprcs:\n",
      "[0.11338166161502039, 0.08765536101650141, 0.11186953503356162, 0.0864128939148339, 0.09519671171816435, 0.07999396285557314, 0.07092202124234837, 0.08737748290671762, 0.08857720648459849, 0.1126983231641521, 0.03636835333317742, 0.09775312492652447]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>aucs</th>\n",
       "      <th>auprcs</th>\n",
       "      <th>props</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FOS</th>\n",
       "      <td>0.903908</td>\n",
       "      <td>0.458555</td>\n",
       "      <td>0.087655</td>\n",
       "      <td>0.096092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JUN</th>\n",
       "      <td>0.904138</td>\n",
       "      <td>0.467772</td>\n",
       "      <td>0.086413</td>\n",
       "      <td>0.095862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYC</th>\n",
       "      <td>0.914253</td>\n",
       "      <td>0.469224</td>\n",
       "      <td>0.079994</td>\n",
       "      <td>0.085747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAX5</th>\n",
       "      <td>0.906207</td>\n",
       "      <td>0.485812</td>\n",
       "      <td>0.088577</td>\n",
       "      <td>0.093793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAX</th>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.499031</td>\n",
       "      <td>0.095197</td>\n",
       "      <td>0.091954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTCF</th>\n",
       "      <td>0.891724</td>\n",
       "      <td>0.501416</td>\n",
       "      <td>0.113382</td>\n",
       "      <td>0.108276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFKB1</th>\n",
       "      <td>0.926437</td>\n",
       "      <td>0.506830</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.073563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GATA2</th>\n",
       "      <td>0.895862</td>\n",
       "      <td>0.509787</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.104138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBP</th>\n",
       "      <td>0.901609</td>\n",
       "      <td>0.511196</td>\n",
       "      <td>0.097753</td>\n",
       "      <td>0.098391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPI1</th>\n",
       "      <td>0.894483</td>\n",
       "      <td>0.517953</td>\n",
       "      <td>0.112698</td>\n",
       "      <td>0.105517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAF1</th>\n",
       "      <td>0.974943</td>\n",
       "      <td>0.521854</td>\n",
       "      <td>0.036368</td>\n",
       "      <td>0.025057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFKB2</th>\n",
       "      <td>0.978391</td>\n",
       "      <td>0.584032</td>\n",
       "      <td>0.087377</td>\n",
       "      <td>0.021609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = np.mean((scores_arr > 0).astype(int) == labels_arr.astype(int), axis=0)\n",
    "\n",
    "print('accuracies:')\n",
    "print(accuracies)\n",
    "\n",
    "import sklearn.metrics\n",
    "print('aucs:')\n",
    "aucs = [sklearn.metrics.roc_auc_score(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(aucs)\n",
    "\n",
    "import sklearn.metrics\n",
    "def prec_recall(ys_true, ys_hat):\n",
    "    ys, xs, thresholds = sklearn.metrics.precision_recall_curve(ys_true, ys_hat)\n",
    "    return sklearn.metrics.auc(xs, ys, reorder=True)\n",
    "\n",
    "print('auprcs:')\n",
    "auprcs = [prec_recall(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(auprcs)\n",
    "\n",
    "props = np.mean(labels_arr, axis=0)\n",
    "\n",
    "import json\n",
    "ids = sorted(json.loads(info['tf_to_pos'].replace(\"'\", '\"')).keys())\n",
    "\n",
    "from IPython.display import display_pretty, display_html\n",
    "import pandas as pd\n",
    "results = pd.DataFrame({'props':props, 'acc':accuracies, 'auprcs':auprcs, 'aucs':aucs}, index=ids).sort_values(by='aucs')\n",
    "display_html(results.to_html(), raw=True)\n",
    "results.to_csv('stats.' + modelname + '.tsv', index_label='id', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
