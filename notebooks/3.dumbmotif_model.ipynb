{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import fxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data locations\n",
    "raw_data_path = '../data/shuffled_examples'\n",
    "outbase = '../output/shuffled_examples'\n",
    "train_out_path = '%s/texttrain' % outbase\n",
    "valid_out_path = '%s/textvalidate' % outbase\n",
    "test_out_path = '%s/texttest' % outbase\n",
    "motif_path = '%s/../motifs' % raw_data_path\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 50\n",
    "num_batches_in_train = int(434786 / batch_size)\n",
    "num_epochs = 2\n",
    "capacity = 2000\n",
    "min_after_dequeue = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# retrieving models\n",
    "global_step_to_load = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are of size 4000\n",
      "outputs are of size 24\n",
      "../output/shuffled_examples/texttest\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# get training data\n",
    "(seq, label), info = fxns.get_seq_and_label(train_out_path)\n",
    "seq_batch, label_batch = tf.train.shuffle_batch([seq, label],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('inputs are of size', info['seq_len'])\n",
    "\n",
    "# get validation data\n",
    "(seqv, labelv), infov = fxns.get_seq_and_label(valid_out_path)\n",
    "seqv_batch, labelv_batch = tf.train.shuffle_batch([seqv, labelv],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('outputs are of size', info['label_len'])\n",
    "\n",
    "reload(fxns)\n",
    "print(test_out_path)\n",
    "(seqt, labelt), info = fxns.get_seq_and_label(test_out_path, num_epochs=None)\n",
    "seqt_batch, labelt_batch = tf.train.shuffle_batch([seqt, labelt],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[[[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]]\n",
      "500.0\n",
      "250.0\n",
      "125.0\n",
      "4000 h 15 125.0\n"
     ]
    }
   ],
   "source": [
    "# # get model -- logistic\n",
    "# import logreg_model\n",
    "# modelname='logreg'\n",
    "# X, Y, loss, logits = logreg_model.get_logreg_model(info['seq_len'], info['label_len'])\n",
    "\n",
    "# get model -- generic convnet\n",
    "import motif_model; reload(motif_model)\n",
    "modelname='dumbmotifconv'\n",
    "#conv_infos = [(7,(1,20),(2,2),4),(5,(1,20),(2,2),7)]#, (8,(1,20),(2,2),5)]\n",
    "conv_infos = [(14,(1,21),(2,2),2,4),(14,(1,8),(2,2),2,14),(7,(1,5),(2,2),2,14)]#, (8,(1,20),(2,2),5)]\n",
    "X, Y, loss, logits = motif_model.get_motif_model(info['seq_len'], info['label_len'], conv_infos, 'dumb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,\n",
    "                                                 name='SGD-Optimizer')\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "# define other summaries we want\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 25 i 24 Avg loss in epoch(incomplete): 0.457716552615\n",
      "Epoch: 0 global_step 50 i 49 Avg loss in epoch(incomplete): 0.313829144835\n",
      "Epoch: 0 global_step 75 i 74 Avg loss in epoch(incomplete): 0.264358419379\n",
      "Epoch: 0 global_step 100 i 99 Avg loss in epoch(incomplete): 0.239746309966\n",
      "Epoch: 0 global_step 125 i 124 Avg loss in epoch(incomplete): 0.224768244267\n",
      "Epoch: 0 global_step 150 i 149 Avg loss in epoch(incomplete): 0.215065217614\n",
      "Epoch: 0 global_step 175 i 174 Avg loss in epoch(incomplete): 0.208021663087\n",
      "Epoch: 0 global_step 200 i 199 Avg loss in epoch(incomplete): 0.202539940476\n",
      "Epoch: 0 global_step 225 i 224 Avg loss in epoch(incomplete): 0.198411289056\n",
      "saving\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.194952221692\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.194952221692\n",
      "Epoch: 0 global_step 275 i 274 Avg loss in epoch(incomplete): 0.192288268967\n",
      "Epoch: 0 global_step 300 i 299 Avg loss in epoch(incomplete): 0.190186198155\n",
      "Epoch: 0 global_step 325 i 324 Avg loss in epoch(incomplete): 0.188293610995\n",
      "Epoch: 0 global_step 350 i 349 Avg loss in epoch(incomplete): 0.186638493155\n",
      "Epoch: 0 global_step 375 i 374 Avg loss in epoch(incomplete): 0.185303637187\n",
      "Epoch: 0 global_step 400 i 399 Avg loss in epoch(incomplete): 0.184082524329\n",
      "Epoch: 0 global_step 425 i 424 Avg loss in epoch(incomplete): 0.18302051993\n",
      "Epoch: 0 global_step 450 i 449 Avg loss in epoch(incomplete): 0.182012208501\n",
      "Epoch: 0 global_step 475 i 474 Avg loss in epoch(incomplete): 0.18110741333\n",
      "saving\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.180297704011\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.180297704011\n",
      "Epoch: 0 global_step 525 i 524 Avg loss in epoch(incomplete): 0.179542219923\n",
      "Epoch: 0 global_step 550 i 549 Avg loss in epoch(incomplete): 0.178878523382\n",
      "Epoch: 0 global_step 575 i 574 Avg loss in epoch(incomplete): 0.178300213036\n",
      "Epoch: 0 global_step 600 i 599 Avg loss in epoch(incomplete): 0.177758689274\n",
      "Epoch: 0 global_step 625 i 624 Avg loss in epoch(incomplete): 0.17728636713\n",
      "Epoch: 0 global_step 650 i 649 Avg loss in epoch(incomplete): 0.176891686939\n",
      "Epoch: 0 global_step 675 i 674 Avg loss in epoch(incomplete): 0.176434768924\n",
      "Epoch: 0 global_step 700 i 699 Avg loss in epoch(incomplete): 0.176040446162\n",
      "Epoch: 0 global_step 725 i 724 Avg loss in epoch(incomplete): 0.175656542141\n",
      "saving\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.175304029405\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.175304029405\n",
      "Epoch: 0 global_step 775 i 774 Avg loss in epoch(incomplete): 0.174989707489\n",
      "Epoch: 0 global_step 800 i 799 Avg loss in epoch(incomplete): 0.174668905046\n",
      "Epoch: 0 global_step 825 i 824 Avg loss in epoch(incomplete): 0.174370686611\n",
      "Epoch: 0 global_step 850 i 849 Avg loss in epoch(incomplete): 0.174095328643\n",
      "Epoch: 0 global_step 875 i 874 Avg loss in epoch(incomplete): 0.173844189882\n",
      "Epoch: 0 global_step 900 i 899 Avg loss in epoch(incomplete): 0.17359177101\n",
      "Epoch: 0 global_step 925 i 924 Avg loss in epoch(incomplete): 0.173370801645\n",
      "Epoch: 0 global_step 950 i 949 Avg loss in epoch(incomplete): 0.173163955039\n",
      "Epoch: 0 global_step 975 i 974 Avg loss in epoch(incomplete): 0.172940286016\n",
      "saving\n",
      "Epoch: 0 global_step 1000 i 999 Avg loss in epoch(incomplete): 0.172722382814\n",
      "Epoch: 0 global_step 1000 i 999 Avg loss in epoch(incomplete): 0.172722382814\n",
      "Epoch: 0 global_step 1025 i 1024 Avg loss in epoch(incomplete): 0.172534279663\n",
      "Epoch: 0 global_step 1050 i 1049 Avg loss in epoch(incomplete): 0.172347080892\n",
      "Epoch: 0 global_step 1075 i 1074 Avg loss in epoch(incomplete): 0.172201289826\n",
      "Epoch: 0 global_step 1100 i 1099 Avg loss in epoch(incomplete): 0.172060058171\n",
      "Epoch: 0 global_step 1125 i 1124 Avg loss in epoch(incomplete): 0.171928181436\n",
      "Epoch: 0 global_step 1150 i 1149 Avg loss in epoch(incomplete): 0.17179359611\n",
      "Epoch: 0 global_step 1175 i 1174 Avg loss in epoch(incomplete): 0.171613244009\n",
      "Epoch: 0 global_step 1200 i 1199 Avg loss in epoch(incomplete): 0.171516198913\n",
      "Epoch: 0 global_step 1225 i 1224 Avg loss in epoch(incomplete): 0.171404632598\n",
      "saving\n",
      "Epoch: 0 global_step 1250 i 1249 Avg loss in epoch(incomplete): 0.171304902875\n",
      "Epoch: 0 global_step 1250 i 1249 Avg loss in epoch(incomplete): 0.171304902875\n",
      "Epoch: 0 global_step 1275 i 1274 Avg loss in epoch(incomplete): 0.171209304333\n",
      "Epoch: 0 global_step 1300 i 1299 Avg loss in epoch(incomplete): 0.171098755816\n",
      "Epoch: 0 global_step 1325 i 1324 Avg loss in epoch(incomplete): 0.171011054718\n",
      "Epoch: 0 global_step 1350 i 1349 Avg loss in epoch(incomplete): 0.170905778055\n",
      "Epoch: 0 global_step 1375 i 1374 Avg loss in epoch(incomplete): 0.170808707822\n",
      "Epoch: 0 global_step 1400 i 1399 Avg loss in epoch(incomplete): 0.17069896434\n",
      "Epoch: 0 global_step 1425 i 1424 Avg loss in epoch(incomplete): 0.170616392315\n",
      "Epoch: 0 global_step 1450 i 1449 Avg loss in epoch(incomplete): 0.170522610555\n",
      "Epoch: 0 global_step 1475 i 1474 Avg loss in epoch(incomplete): 0.170426844732\n",
      "saving\n",
      "Epoch: 0 global_step 1500 i 1499 Avg loss in epoch(incomplete): 0.170363898804\n",
      "Epoch: 0 global_step 1500 i 1499 Avg loss in epoch(incomplete): 0.170363898804\n",
      "Epoch: 0 global_step 1525 i 1524 Avg loss in epoch(incomplete): 0.170274441956\n",
      "Epoch: 0 global_step 1550 i 1549 Avg loss in epoch(incomplete): 0.170183802199\n",
      "Epoch: 0 global_step 1575 i 1574 Avg loss in epoch(incomplete): 0.170101814989\n",
      "Epoch: 0 global_step 1600 i 1599 Avg loss in epoch(incomplete): 0.170024560336\n",
      "Epoch: 0 global_step 1625 i 1624 Avg loss in epoch(incomplete): 0.169952055115\n",
      "Epoch: 0 global_step 1650 i 1649 Avg loss in epoch(incomplete): 0.169864257538\n",
      "Epoch: 0 global_step 1675 i 1674 Avg loss in epoch(incomplete): 0.169782022487\n",
      "Epoch: 0 global_step 1700 i 1699 Avg loss in epoch(incomplete): 0.169712306682\n",
      "Epoch: 0 global_step 1725 i 1724 Avg loss in epoch(incomplete): 0.169640601027\n",
      "saving\n",
      "Epoch: 0 global_step 1750 i 1749 Avg loss in epoch(incomplete): 0.16957061679\n",
      "Epoch: 0 global_step 1750 i 1749 Avg loss in epoch(incomplete): 0.16957061679\n",
      "Epoch: 0 global_step 1775 i 1774 Avg loss in epoch(incomplete): 0.169494307629\n",
      "Epoch: 0 global_step 1800 i 1799 Avg loss in epoch(incomplete): 0.169440274197\n",
      "Epoch: 0 global_step 1825 i 1824 Avg loss in epoch(incomplete): 0.169392497784\n",
      "Epoch: 0 global_step 1850 i 1849 Avg loss in epoch(incomplete): 0.169334175023\n",
      "Epoch: 0 global_step 1875 i 1874 Avg loss in epoch(incomplete): 0.16927636617\n",
      "Epoch: 0 global_step 1900 i 1899 Avg loss in epoch(incomplete): 0.169228051093\n",
      "Epoch: 0 global_step 1925 i 1924 Avg loss in epoch(incomplete): 0.169178734855\n",
      "Epoch: 0 global_step 1950 i 1949 Avg loss in epoch(incomplete): 0.169113151966\n",
      "Epoch: 0 global_step 1975 i 1974 Avg loss in epoch(incomplete): 0.169046214826\n",
      "saving\n",
      "Epoch: 0 global_step 2000 i 1999 Avg loss in epoch(incomplete): 0.16899864953\n",
      "Epoch: 0 global_step 2000 i 1999 Avg loss in epoch(incomplete): 0.16899864953\n",
      "Epoch: 0 global_step 2025 i 2024 Avg loss in epoch(incomplete): 0.168940926584\n",
      "Epoch: 0 global_step 2050 i 2049 Avg loss in epoch(incomplete): 0.16888869777\n",
      "Epoch: 0 global_step 2075 i 2074 Avg loss in epoch(incomplete): 0.168837533988\n",
      "Epoch: 0 global_step 2100 i 2099 Avg loss in epoch(incomplete): 0.168779680083\n",
      "Epoch: 0 global_step 2125 i 2124 Avg loss in epoch(incomplete): 0.168714672285\n",
      "Epoch: 0 global_step 2150 i 2149 Avg loss in epoch(incomplete): 0.168669455155\n",
      "Epoch: 0 global_step 2175 i 2174 Avg loss in epoch(incomplete): 0.168631997814\n",
      "Epoch: 0 global_step 2200 i 2199 Avg loss in epoch(incomplete): 0.16859583661\n",
      "Epoch: 0 global_step 2225 i 2224 Avg loss in epoch(incomplete): 0.168557697622\n",
      "saving\n",
      "Epoch: 0 global_step 2250 i 2249 Avg loss in epoch(incomplete): 0.168521119555\n",
      "Epoch: 0 global_step 2250 i 2249 Avg loss in epoch(incomplete): 0.168521119555\n",
      "Epoch: 0 global_step 2275 i 2274 Avg loss in epoch(incomplete): 0.168485659577\n",
      "Epoch: 0 global_step 2300 i 2299 Avg loss in epoch(incomplete): 0.168441923157\n",
      "Epoch: 0 global_step 2325 i 2324 Avg loss in epoch(incomplete): 0.168398850528\n",
      "Epoch: 0 global_step 2350 i 2349 Avg loss in epoch(incomplete): 0.168356138322\n",
      "Epoch: 0 global_step 2375 i 2374 Avg loss in epoch(incomplete): 0.168323796919\n",
      "Epoch: 0 global_step 2400 i 2399 Avg loss in epoch(incomplete): 0.168291369112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 2425 i 2424 Avg loss in epoch(incomplete): 0.168261210531\n",
      "Epoch: 0 global_step 2450 i 2449 Avg loss in epoch(incomplete): 0.168235201708\n",
      "Epoch: 0 global_step 2475 i 2474 Avg loss in epoch(incomplete): 0.168192652921\n",
      "saving\n",
      "Epoch: 0 global_step 2500 i 2499 Avg loss in epoch(incomplete): 0.168161653554\n",
      "Epoch: 0 global_step 2500 i 2499 Avg loss in epoch(incomplete): 0.168161653554\n",
      "Epoch: 0 global_step 2525 i 2524 Avg loss in epoch(incomplete): 0.168133609136\n",
      "Epoch: 0 global_step 2550 i 2549 Avg loss in epoch(incomplete): 0.168099024693\n",
      "Epoch: 0 global_step 2575 i 2574 Avg loss in epoch(incomplete): 0.168067243632\n",
      "Epoch: 0 global_step 2600 i 2599 Avg loss in epoch(incomplete): 0.168041149653\n",
      "Epoch: 0 global_step 2625 i 2624 Avg loss in epoch(incomplete): 0.167999432002\n",
      "Epoch: 0 global_step 2650 i 2649 Avg loss in epoch(incomplete): 0.167972691188\n",
      "Epoch: 0 global_step 2675 i 2674 Avg loss in epoch(incomplete): 0.167937030809\n",
      "Epoch: 0 global_step 2700 i 2699 Avg loss in epoch(incomplete): 0.167909805272\n",
      "Epoch: 0 global_step 2725 i 2724 Avg loss in epoch(incomplete): 0.167887228049\n",
      "saving\n",
      "Epoch: 0 global_step 2750 i 2749 Avg loss in epoch(incomplete): 0.167859292865\n",
      "Epoch: 0 global_step 2750 i 2749 Avg loss in epoch(incomplete): 0.167859292865\n",
      "Epoch: 0 global_step 2775 i 2774 Avg loss in epoch(incomplete): 0.167829286467\n",
      "Epoch: 0 global_step 2800 i 2799 Avg loss in epoch(incomplete): 0.167801610327\n",
      "Epoch: 0 global_step 2825 i 2824 Avg loss in epoch(incomplete): 0.167765168817\n",
      "Epoch: 0 global_step 2850 i 2849 Avg loss in epoch(incomplete): 0.167738533966\n",
      "Epoch: 0 global_step 2875 i 2874 Avg loss in epoch(incomplete): 0.167715849415\n",
      "Epoch: 0 global_step 2900 i 2899 Avg loss in epoch(incomplete): 0.16769143621\n",
      "Epoch: 0 global_step 2925 i 2924 Avg loss in epoch(incomplete): 0.167664242952\n",
      "Epoch: 0 global_step 2950 i 2949 Avg loss in epoch(incomplete): 0.167643997487\n",
      "Epoch: 0 global_step 2975 i 2974 Avg loss in epoch(incomplete): 0.167618720341\n",
      "saving\n",
      "Epoch: 0 global_step 3000 i 2999 Avg loss in epoch(incomplete): 0.16759286646\n",
      "Epoch: 0 global_step 3000 i 2999 Avg loss in epoch(incomplete): 0.16759286646\n",
      "Epoch: 0 global_step 3025 i 3024 Avg loss in epoch(incomplete): 0.16756098767\n",
      "Epoch: 0 global_step 3050 i 3049 Avg loss in epoch(incomplete): 0.167543790209\n",
      "Epoch: 0 global_step 3075 i 3074 Avg loss in epoch(incomplete): 0.167524917431\n",
      "Epoch: 0 global_step 3100 i 3099 Avg loss in epoch(incomplete): 0.167504694135\n",
      "Epoch: 0 global_step 3125 i 3124 Avg loss in epoch(incomplete): 0.167481584282\n",
      "Epoch: 0 global_step 3150 i 3149 Avg loss in epoch(incomplete): 0.167450595011\n",
      "Epoch: 0 global_step 3175 i 3174 Avg loss in epoch(incomplete): 0.16742381852\n",
      "Epoch: 0 global_step 3200 i 3199 Avg loss in epoch(incomplete): 0.167403248409\n",
      "Epoch: 0 global_step 3225 i 3224 Avg loss in epoch(incomplete): 0.167386749378\n",
      "saving\n",
      "Epoch: 0 global_step 3250 i 3249 Avg loss in epoch(incomplete): 0.167361225362\n",
      "Epoch: 0 global_step 3250 i 3249 Avg loss in epoch(incomplete): 0.167361225362\n",
      "Epoch: 0 global_step 3275 i 3274 Avg loss in epoch(incomplete): 0.167349394878\n",
      "Epoch: 0 global_step 3300 i 3299 Avg loss in epoch(incomplete): 0.167327041581\n",
      "Epoch: 0 global_step 3325 i 3324 Avg loss in epoch(incomplete): 0.167311059214\n",
      "Epoch: 0 global_step 3350 i 3349 Avg loss in epoch(incomplete): 0.167285010441\n",
      "Epoch: 0 global_step 3375 i 3374 Avg loss in epoch(incomplete): 0.167259344264\n",
      "Epoch: 0 global_step 3400 i 3399 Avg loss in epoch(incomplete): 0.167232512442\n",
      "Epoch: 0 global_step 3425 i 3424 Avg loss in epoch(incomplete): 0.167209518017\n",
      "Epoch: 0 global_step 3450 i 3449 Avg loss in epoch(incomplete): 0.16718539996\n",
      "Epoch: 0 global_step 3475 i 3474 Avg loss in epoch(incomplete): 0.167161093479\n",
      "saving\n",
      "Epoch: 0 global_step 3500 i 3499 Avg loss in epoch(incomplete): 0.167134791698\n",
      "Epoch: 0 global_step 3500 i 3499 Avg loss in epoch(incomplete): 0.167134791698\n",
      "Epoch: 0 global_step 3525 i 3524 Avg loss in epoch(incomplete): 0.167109114718\n",
      "Epoch: 0 global_step 3550 i 3549 Avg loss in epoch(incomplete): 0.167083270894\n",
      "Epoch: 0 global_step 3575 i 3574 Avg loss in epoch(incomplete): 0.167063015474\n",
      "Epoch: 0 global_step 3600 i 3599 Avg loss in epoch(incomplete): 0.167047737779\n",
      "Epoch: 0 global_step 3625 i 3624 Avg loss in epoch(incomplete): 0.167025236405\n",
      "Epoch: 0 global_step 3650 i 3649 Avg loss in epoch(incomplete): 0.167007147384\n",
      "Epoch: 0 global_step 3675 i 3674 Avg loss in epoch(incomplete): 0.166982189384\n",
      "Epoch: 0 global_step 3700 i 3699 Avg loss in epoch(incomplete): 0.166962555244\n",
      "Epoch: 0 global_step 3725 i 3724 Avg loss in epoch(incomplete): 0.166933447315\n",
      "saving\n",
      "Epoch: 0 global_step 3750 i 3749 Avg loss in epoch(incomplete): 0.166918069685\n",
      "Epoch: 0 global_step 3750 i 3749 Avg loss in epoch(incomplete): 0.166918069685\n",
      "Epoch: 0 global_step 3775 i 3774 Avg loss in epoch(incomplete): 0.166907206418\n",
      "Epoch: 0 global_step 3800 i 3799 Avg loss in epoch(incomplete): 0.166888349068\n",
      "Epoch: 0 global_step 3825 i 3824 Avg loss in epoch(incomplete): 0.166870944145\n",
      "Epoch: 0 global_step 3850 i 3849 Avg loss in epoch(incomplete): 0.166837073398\n",
      "Epoch: 0 global_step 3875 i 3874 Avg loss in epoch(incomplete): 0.16681716907\n",
      "Epoch: 0 global_step 3900 i 3899 Avg loss in epoch(incomplete): 0.16679545798\n",
      "Epoch: 0 global_step 3925 i 3924 Avg loss in epoch(incomplete): 0.166775941879\n",
      "Epoch: 0 global_step 3950 i 3949 Avg loss in epoch(incomplete): 0.166756402084\n",
      "Epoch: 0 global_step 3975 i 3974 Avg loss in epoch(incomplete): 0.166736243566\n",
      "saving\n",
      "Epoch: 0 global_step 4000 i 3999 Avg loss in epoch(incomplete): 0.166713368766\n",
      "Epoch: 0 global_step 4000 i 3999 Avg loss in epoch(incomplete): 0.166713368766\n",
      "Epoch: 0 global_step 4025 i 4024 Avg loss in epoch(incomplete): 0.166700484949\n",
      "Epoch: 0 global_step 4050 i 4049 Avg loss in epoch(incomplete): 0.166684893852\n",
      "Epoch: 0 global_step 4075 i 4074 Avg loss in epoch(incomplete): 0.166657988001\n",
      "Epoch: 0 global_step 4100 i 4099 Avg loss in epoch(incomplete): 0.166642685354\n",
      "Epoch: 0 global_step 4125 i 4124 Avg loss in epoch(incomplete): 0.166620236173\n",
      "Epoch: 0 global_step 4150 i 4149 Avg loss in epoch(incomplete): 0.166609255388\n",
      "Epoch: 0 global_step 4175 i 4174 Avg loss in epoch(incomplete): 0.166588031999\n",
      "Epoch: 0 global_step 4200 i 4199 Avg loss in epoch(incomplete): 0.166563027274\n",
      "Epoch: 0 global_step 4225 i 4224 Avg loss in epoch(incomplete): 0.166543604816\n",
      "saving\n",
      "Epoch: 0 global_step 4250 i 4249 Avg loss in epoch(incomplete): 0.166518426786\n",
      "Epoch: 0 global_step 4250 i 4249 Avg loss in epoch(incomplete): 0.166518426786\n",
      "Epoch: 0 global_step 4275 i 4274 Avg loss in epoch(incomplete): 0.166503042774\n",
      "Epoch: 0 global_step 4300 i 4299 Avg loss in epoch(incomplete): 0.166486160069\n",
      "Epoch: 0 global_step 4325 i 4324 Avg loss in epoch(incomplete): 0.16646752743\n",
      "Epoch: 0 global_step 4350 i 4349 Avg loss in epoch(incomplete): 0.166455014989\n",
      "Epoch: 0 global_step 4375 i 4374 Avg loss in epoch(incomplete): 0.166435071771\n",
      "Epoch: 0 global_step 4400 i 4399 Avg loss in epoch(incomplete): 0.166418793415\n",
      "Epoch: 0 global_step 4425 i 4424 Avg loss in epoch(incomplete): 0.166400825883\n",
      "Epoch: 0 global_step 4450 i 4449 Avg loss in epoch(incomplete): 0.16638593584\n",
      "Epoch: 0 global_step 4475 i 4474 Avg loss in epoch(incomplete): 0.166367800992\n",
      "saving\n",
      "Epoch: 0 global_step 4500 i 4499 Avg loss in epoch(incomplete): 0.166347549385\n",
      "Epoch: 0 global_step 4500 i 4499 Avg loss in epoch(incomplete): 0.166347549385\n",
      "Epoch: 0 global_step 4525 i 4524 Avg loss in epoch(incomplete): 0.166330563031\n",
      "Epoch: 0 global_step 4550 i 4549 Avg loss in epoch(incomplete): 0.166314332397\n",
      "Epoch: 0 global_step 4575 i 4574 Avg loss in epoch(incomplete): 0.166290557873\n",
      "Epoch: 0 global_step 4600 i 4599 Avg loss in epoch(incomplete): 0.166273480213\n",
      "Epoch: 0 global_step 4625 i 4624 Avg loss in epoch(incomplete): 0.166252048476\n",
      "Epoch: 0 global_step 4650 i 4649 Avg loss in epoch(incomplete): 0.166231806445\n",
      "Epoch: 0 global_step 4675 i 4674 Avg loss in epoch(incomplete): 0.166215271532\n",
      "Epoch: 0 global_step 4700 i 4699 Avg loss in epoch(incomplete): 0.166195696339\n",
      "Epoch: 0 global_step 4725 i 4724 Avg loss in epoch(incomplete): 0.166181045682\n",
      "saving\n",
      "Epoch: 0 global_step 4750 i 4749 Avg loss in epoch(incomplete): 0.166160496119\n",
      "Epoch: 0 global_step 4750 i 4749 Avg loss in epoch(incomplete): 0.166160496119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 4775 i 4774 Avg loss in epoch(incomplete): 0.166150248893\n",
      "Epoch: 0 global_step 4800 i 4799 Avg loss in epoch(incomplete): 0.166137077386\n",
      "Epoch: 0 global_step 4825 i 4824 Avg loss in epoch(incomplete): 0.16612235652\n",
      "Epoch: 0 global_step 4850 i 4849 Avg loss in epoch(incomplete): 0.166109750102\n",
      "Epoch: 0 global_step 4875 i 4874 Avg loss in epoch(incomplete): 0.166094979002\n",
      "Epoch: 0 global_step 4900 i 4899 Avg loss in epoch(incomplete): 0.166078040125\n",
      "Epoch: 0 global_step 4925 i 4924 Avg loss in epoch(incomplete): 0.166060635463\n",
      "Epoch: 0 global_step 4950 i 4949 Avg loss in epoch(incomplete): 0.166043746625\n",
      "Epoch: 0 global_step 4975 i 4974 Avg loss in epoch(incomplete): 0.166022823504\n",
      "saving\n",
      "Epoch: 0 global_step 5000 i 4999 Avg loss in epoch(incomplete): 0.16600438647\n",
      "Epoch: 0 global_step 5000 i 4999 Avg loss in epoch(incomplete): 0.16600438647\n",
      "Epoch: 0 global_step 5025 i 5024 Avg loss in epoch(incomplete): 0.165980784312\n",
      "Epoch: 0 global_step 5050 i 5049 Avg loss in epoch(incomplete): 0.16596340432\n",
      "Epoch: 0 global_step 5075 i 5074 Avg loss in epoch(incomplete): 0.165949144531\n",
      "Epoch: 0 global_step 5100 i 5099 Avg loss in epoch(incomplete): 0.165926502543\n",
      "Epoch: 0 global_step 5125 i 5124 Avg loss in epoch(incomplete): 0.16590867893\n",
      "Epoch: 0 global_step 5150 i 5149 Avg loss in epoch(incomplete): 0.165885828346\n",
      "Epoch: 0 global_step 5175 i 5174 Avg loss in epoch(incomplete): 0.165871993027\n",
      "Epoch: 0 global_step 5200 i 5199 Avg loss in epoch(incomplete): 0.165854847669\n",
      "Epoch: 0 global_step 5225 i 5224 Avg loss in epoch(incomplete): 0.165834273722\n",
      "saving\n",
      "Epoch: 0 global_step 5250 i 5249 Avg loss in epoch(incomplete): 0.165816043695\n",
      "Epoch: 0 global_step 5250 i 5249 Avg loss in epoch(incomplete): 0.165816043695\n",
      "Epoch: 0 global_step 5275 i 5274 Avg loss in epoch(incomplete): 0.165805079129\n",
      "Epoch: 0 global_step 5300 i 5299 Avg loss in epoch(incomplete): 0.165779021686\n",
      "Epoch: 0 global_step 5325 i 5324 Avg loss in epoch(incomplete): 0.165761149593\n",
      "Epoch: 0 global_step 5350 i 5349 Avg loss in epoch(incomplete): 0.165741479085\n",
      "Epoch: 0 global_step 5375 i 5374 Avg loss in epoch(incomplete): 0.165713782019\n",
      "Epoch: 0 global_step 5400 i 5399 Avg loss in epoch(incomplete): 0.165697789148\n",
      "Epoch: 0 global_step 5425 i 5424 Avg loss in epoch(incomplete): 0.165682508553\n",
      "Epoch: 0 global_step 5450 i 5449 Avg loss in epoch(incomplete): 0.165668497687\n",
      "Epoch: 0 global_step 5475 i 5474 Avg loss in epoch(incomplete): 0.165659315583\n",
      "saving\n",
      "Epoch: 0 global_step 5500 i 5499 Avg loss in epoch(incomplete): 0.165643959639\n",
      "Epoch: 0 global_step 5500 i 5499 Avg loss in epoch(incomplete): 0.165643959639\n",
      "Epoch: 0 global_step 5525 i 5524 Avg loss in epoch(incomplete): 0.165629410064\n",
      "Epoch: 0 global_step 5550 i 5549 Avg loss in epoch(incomplete): 0.165608360512\n",
      "Epoch: 0 global_step 5575 i 5574 Avg loss in epoch(incomplete): 0.165593604659\n",
      "Epoch: 0 global_step 5600 i 5599 Avg loss in epoch(incomplete): 0.165578422685\n",
      "Epoch: 0 global_step 5625 i 5624 Avg loss in epoch(incomplete): 0.165564131525\n",
      "Epoch: 0 global_step 5650 i 5649 Avg loss in epoch(incomplete): 0.165547338853\n",
      "Epoch: 0 global_step 5675 i 5674 Avg loss in epoch(incomplete): 0.165532013979\n",
      "Epoch: 0 global_step 5700 i 5699 Avg loss in epoch(incomplete): 0.165519983512\n",
      "Epoch: 0 global_step 5725 i 5724 Avg loss in epoch(incomplete): 0.165507248575\n",
      "saving\n",
      "Epoch: 0 global_step 5750 i 5749 Avg loss in epoch(incomplete): 0.165489523538\n",
      "Epoch: 0 global_step 5750 i 5749 Avg loss in epoch(incomplete): 0.165489523538\n",
      "Epoch: 0 global_step 5775 i 5774 Avg loss in epoch(incomplete): 0.165480357024\n",
      "Epoch: 0 global_step 5800 i 5799 Avg loss in epoch(incomplete): 0.165463257483\n",
      "Epoch: 0 global_step 5825 i 5824 Avg loss in epoch(incomplete): 0.165447458817\n",
      "Epoch: 0 global_step 5850 i 5849 Avg loss in epoch(incomplete): 0.165424688458\n",
      "Epoch: 0 global_step 5875 i 5874 Avg loss in epoch(incomplete): 0.165410795767\n",
      "Epoch: 0 global_step 5900 i 5899 Avg loss in epoch(incomplete): 0.165394637198\n",
      "Epoch: 0 global_step 5925 i 5924 Avg loss in epoch(incomplete): 0.165379590953\n",
      "Epoch: 0 global_step 5950 i 5949 Avg loss in epoch(incomplete): 0.165367485691\n",
      "Epoch: 0 global_step 5975 i 5974 Avg loss in epoch(incomplete): 0.165349059616\n",
      "saving\n",
      "Epoch: 0 global_step 6000 i 5999 Avg loss in epoch(incomplete): 0.165340235732\n",
      "Epoch: 0 global_step 6000 i 5999 Avg loss in epoch(incomplete): 0.165340235732\n",
      "Epoch: 0 global_step 6025 i 6024 Avg loss in epoch(incomplete): 0.165322132702\n",
      "Epoch: 0 global_step 6050 i 6049 Avg loss in epoch(incomplete): 0.165309249894\n",
      "Epoch: 0 global_step 6075 i 6074 Avg loss in epoch(incomplete): 0.165292511237\n",
      "Epoch: 0 global_step 6100 i 6099 Avg loss in epoch(incomplete): 0.165272788554\n",
      "Epoch: 0 global_step 6125 i 6124 Avg loss in epoch(incomplete): 0.165252044096\n",
      "Epoch: 0 global_step 6150 i 6149 Avg loss in epoch(incomplete): 0.165230783975\n",
      "Epoch: 0 global_step 6175 i 6174 Avg loss in epoch(incomplete): 0.165222194159\n",
      "Epoch: 0 global_step 6200 i 6199 Avg loss in epoch(incomplete): 0.165206092776\n",
      "Epoch: 0 global_step 6225 i 6224 Avg loss in epoch(incomplete): 0.16518729438\n",
      "saving\n",
      "Epoch: 0 global_step 6250 i 6249 Avg loss in epoch(incomplete): 0.165170542793\n",
      "Epoch: 0 global_step 6250 i 6249 Avg loss in epoch(incomplete): 0.165170542793\n",
      "Epoch: 0 global_step 6275 i 6274 Avg loss in epoch(incomplete): 0.165152610431\n",
      "Epoch: 0 global_step 6300 i 6299 Avg loss in epoch(incomplete): 0.165133665467\n",
      "Epoch: 0 global_step 6325 i 6324 Avg loss in epoch(incomplete): 0.165117682869\n",
      "Epoch: 0 global_step 6350 i 6349 Avg loss in epoch(incomplete): 0.165105768686\n",
      "Epoch: 0 global_step 6375 i 6374 Avg loss in epoch(incomplete): 0.165091557818\n",
      "Epoch: 0 global_step 6400 i 6399 Avg loss in epoch(incomplete): 0.165082063023\n",
      "Epoch: 0 global_step 6425 i 6424 Avg loss in epoch(incomplete): 0.165064976392\n",
      "Epoch: 0 global_step 6450 i 6449 Avg loss in epoch(incomplete): 0.165047855359\n",
      "Epoch: 0 global_step 6475 i 6474 Avg loss in epoch(incomplete): 0.165032790602\n",
      "saving\n",
      "Epoch: 0 global_step 6500 i 6499 Avg loss in epoch(incomplete): 0.165014591261\n",
      "Epoch: 0 global_step 6500 i 6499 Avg loss in epoch(incomplete): 0.165014591261\n",
      "Epoch: 0 global_step 6525 i 6524 Avg loss in epoch(incomplete): 0.164998366547\n",
      "Epoch: 0 global_step 6550 i 6549 Avg loss in epoch(incomplete): 0.16498207745\n",
      "Epoch: 0 global_step 6575 i 6574 Avg loss in epoch(incomplete): 0.164965426809\n",
      "Epoch: 0 global_step 6600 i 6599 Avg loss in epoch(incomplete): 0.164950422419\n",
      "Epoch: 0 global_step 6625 i 6624 Avg loss in epoch(incomplete): 0.164929583702\n",
      "Epoch: 0 global_step 6650 i 6649 Avg loss in epoch(incomplete): 0.16491332176\n",
      "Epoch: 0 global_step 6675 i 6674 Avg loss in epoch(incomplete): 0.164903215842\n",
      "Epoch: 0 global_step 6700 i 6699 Avg loss in epoch(incomplete): 0.164886898018\n",
      "Epoch: 0 global_step 6725 i 6724 Avg loss in epoch(incomplete): 0.164871077653\n",
      "saving\n",
      "Epoch: 0 global_step 6750 i 6749 Avg loss in epoch(incomplete): 0.164856769557\n",
      "Epoch: 0 global_step 6750 i 6749 Avg loss in epoch(incomplete): 0.164856769557\n",
      "Epoch: 0 global_step 6775 i 6774 Avg loss in epoch(incomplete): 0.164844158591\n",
      "Epoch: 0 global_step 6800 i 6799 Avg loss in epoch(incomplete): 0.164829176325\n",
      "Epoch: 0 global_step 6825 i 6824 Avg loss in epoch(incomplete): 0.164814374488\n",
      "Epoch: 0 global_step 6850 i 6849 Avg loss in epoch(incomplete): 0.164799882443\n",
      "Epoch: 0 global_step 6875 i 6874 Avg loss in epoch(incomplete): 0.164784320133\n",
      "Epoch: 0 global_step 6900 i 6899 Avg loss in epoch(incomplete): 0.164772268408\n",
      "Epoch: 0 global_step 6925 i 6924 Avg loss in epoch(incomplete): 0.164759667931\n",
      "Epoch: 0 global_step 6950 i 6949 Avg loss in epoch(incomplete): 0.164748734167\n",
      "Epoch: 0 global_step 6975 i 6974 Avg loss in epoch(incomplete): 0.164739700344\n",
      "saving\n",
      "Epoch: 0 global_step 7000 i 6999 Avg loss in epoch(incomplete): 0.164725090257\n",
      "Epoch: 0 global_step 7000 i 6999 Avg loss in epoch(incomplete): 0.164725090257\n",
      "Epoch: 0 global_step 7025 i 7024 Avg loss in epoch(incomplete): 0.164709484125\n",
      "Epoch: 0 global_step 7050 i 7049 Avg loss in epoch(incomplete): 0.164701146629\n",
      "Epoch: 0 global_step 7075 i 7074 Avg loss in epoch(incomplete): 0.16468871679\n",
      "Epoch: 0 global_step 7100 i 7099 Avg loss in epoch(incomplete): 0.164670979267\n",
      "Epoch: 0 global_step 7125 i 7124 Avg loss in epoch(incomplete): 0.164655162989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 7150 i 7149 Avg loss in epoch(incomplete): 0.16464297037\n",
      "Epoch: 0 global_step 7175 i 7174 Avg loss in epoch(incomplete): 0.164630347872\n",
      "Epoch: 0 global_step 7200 i 7199 Avg loss in epoch(incomplete): 0.164618962889\n",
      "Epoch: 0 global_step 7225 i 7224 Avg loss in epoch(incomplete): 0.164607437498\n",
      "saving\n",
      "Epoch: 0 global_step 7250 i 7249 Avg loss in epoch(incomplete): 0.164593803003\n",
      "Epoch: 0 global_step 7250 i 7249 Avg loss in epoch(incomplete): 0.164593803003\n",
      "Epoch: 0 global_step 7275 i 7274 Avg loss in epoch(incomplete): 0.164581495216\n",
      "Epoch: 0 global_step 7300 i 7299 Avg loss in epoch(incomplete): 0.164568590254\n",
      "Epoch: 0 global_step 7325 i 7324 Avg loss in epoch(incomplete): 0.16455606477\n",
      "Epoch: 0 global_step 7350 i 7349 Avg loss in epoch(incomplete): 0.164538365653\n",
      "Epoch: 0 global_step 7375 i 7374 Avg loss in epoch(incomplete): 0.164523692169\n",
      "Epoch: 0 global_step 7400 i 7399 Avg loss in epoch(incomplete): 0.164513827156\n",
      "Epoch: 0 global_step 7425 i 7424 Avg loss in epoch(incomplete): 0.164501189216\n",
      "Epoch: 0 global_step 7450 i 7449 Avg loss in epoch(incomplete): 0.164492655292\n",
      "Epoch: 0 global_step 7475 i 7474 Avg loss in epoch(incomplete): 0.164478015409\n",
      "saving\n",
      "Epoch: 0 global_step 7500 i 7499 Avg loss in epoch(incomplete): 0.164466885495\n",
      "Epoch: 0 global_step 7500 i 7499 Avg loss in epoch(incomplete): 0.164466885495\n",
      "Epoch: 0 global_step 7525 i 7524 Avg loss in epoch(incomplete): 0.164453120346\n",
      "Epoch: 0 global_step 7550 i 7549 Avg loss in epoch(incomplete): 0.164442186673\n",
      "Epoch: 0 global_step 7575 i 7574 Avg loss in epoch(incomplete): 0.164428500307\n",
      "Epoch: 0 global_step 7600 i 7599 Avg loss in epoch(incomplete): 0.164416042541\n",
      "Epoch: 0 global_step 7625 i 7624 Avg loss in epoch(incomplete): 0.164408036109\n",
      "Epoch: 0 global_step 7650 i 7649 Avg loss in epoch(incomplete): 0.164401347538\n",
      "Epoch: 0 global_step 7675 i 7674 Avg loss in epoch(incomplete): 0.164390934989\n",
      "Epoch: 0 global_step 7700 i 7699 Avg loss in epoch(incomplete): 0.16437543817\n",
      "Epoch: 0 global_step 7725 i 7724 Avg loss in epoch(incomplete): 0.164362532518\n",
      "saving\n",
      "Epoch: 0 global_step 7750 i 7749 Avg loss in epoch(incomplete): 0.16434755133\n",
      "Epoch: 0 global_step 7750 i 7749 Avg loss in epoch(incomplete): 0.16434755133\n",
      "Epoch: 0 global_step 7775 i 7774 Avg loss in epoch(incomplete): 0.164336011394\n",
      "Epoch: 0 global_step 7800 i 7799 Avg loss in epoch(incomplete): 0.164320921157\n",
      "Epoch: 0 global_step 7825 i 7824 Avg loss in epoch(incomplete): 0.164311174385\n",
      "Epoch: 0 global_step 7850 i 7849 Avg loss in epoch(incomplete): 0.164301362265\n",
      "Epoch: 0 global_step 7875 i 7874 Avg loss in epoch(incomplete): 0.164290841303\n",
      "Epoch: 0 global_step 7900 i 7899 Avg loss in epoch(incomplete): 0.16427745678\n",
      "Epoch: 0 global_step 7925 i 7924 Avg loss in epoch(incomplete): 0.164263447899\n",
      "Epoch: 0 global_step 7950 i 7949 Avg loss in epoch(incomplete): 0.164251709837\n",
      "Epoch: 0 global_step 7975 i 7974 Avg loss in epoch(incomplete): 0.164242009481\n",
      "saving\n",
      "Epoch: 0 global_step 8000 i 7999 Avg loss in epoch(incomplete): 0.164230077269\n",
      "Epoch: 0 global_step 8000 i 7999 Avg loss in epoch(incomplete): 0.164230077269\n",
      "Epoch: 0 global_step 8025 i 8024 Avg loss in epoch(incomplete): 0.164224201079\n",
      "Epoch: 0 global_step 8050 i 8049 Avg loss in epoch(incomplete): 0.164212162513\n",
      "Epoch: 0 global_step 8075 i 8074 Avg loss in epoch(incomplete): 0.164201745396\n",
      "Epoch: 0 global_step 8100 i 8099 Avg loss in epoch(incomplete): 0.16418893941\n",
      "Epoch: 0 global_step 8125 i 8124 Avg loss in epoch(incomplete): 0.1641819489\n",
      "Epoch: 0 global_step 8150 i 8149 Avg loss in epoch(incomplete): 0.164170773865\n",
      "Epoch: 0 global_step 8175 i 8174 Avg loss in epoch(incomplete): 0.164154645496\n",
      "Epoch: 0 global_step 8200 i 8199 Avg loss in epoch(incomplete): 0.164141934937\n",
      "Epoch: 0 global_step 8225 i 8224 Avg loss in epoch(incomplete): 0.164131697672\n",
      "saving\n",
      "Epoch: 0 global_step 8250 i 8249 Avg loss in epoch(incomplete): 0.16412096912\n",
      "Epoch: 0 global_step 8250 i 8249 Avg loss in epoch(incomplete): 0.16412096912\n",
      "Epoch: 0 global_step 8275 i 8274 Avg loss in epoch(incomplete): 0.164111042693\n",
      "Epoch: 0 global_step 8300 i 8299 Avg loss in epoch(incomplete): 0.16410527018\n",
      "Epoch: 0 global_step 8325 i 8324 Avg loss in epoch(incomplete): 0.164093033204\n",
      "Epoch: 0 global_step 8350 i 8349 Avg loss in epoch(incomplete): 0.164079434038\n",
      "Epoch: 0 global_step 8375 i 8374 Avg loss in epoch(incomplete): 0.164068402813\n",
      "Epoch: 0 global_step 8400 i 8399 Avg loss in epoch(incomplete): 0.16405428157\n",
      "Epoch: 0 global_step 8425 i 8424 Avg loss in epoch(incomplete): 0.164042169068\n",
      "Epoch: 0 global_step 8450 i 8449 Avg loss in epoch(incomplete): 0.164033973799\n",
      "Epoch: 0 global_step 8475 i 8474 Avg loss in epoch(incomplete): 0.164023622092\n",
      "saving\n",
      "Epoch: 0 global_step 8500 i 8499 Avg loss in epoch(incomplete): 0.164013737992\n",
      "Epoch: 0 global_step 8500 i 8499 Avg loss in epoch(incomplete): 0.164013737992\n",
      "Epoch: 0 global_step 8525 i 8524 Avg loss in epoch(incomplete): 0.164000299575\n",
      "Epoch: 0 global_step 8550 i 8549 Avg loss in epoch(incomplete): 0.163990378594\n",
      "Epoch: 0 global_step 8575 i 8574 Avg loss in epoch(incomplete): 0.16397830097\n",
      "Epoch: 0 global_step 8600 i 8599 Avg loss in epoch(incomplete): 0.163964166435\n",
      "Epoch: 0 global_step 8625 i 8624 Avg loss in epoch(incomplete): 0.163952378629\n",
      "Epoch: 0 global_step 8650 i 8649 Avg loss in epoch(incomplete): 0.163940376576\n",
      "Epoch: 0 global_step 8675 i 8674 Avg loss in epoch(incomplete): 0.16392960642\n",
      "saving\n",
      "Epoch: 0 global_step 8695 i 8694 Avg loss in epoch(incomplete): 0.163919807144\n",
      "Epoch: 1 global_step 8700 i 4 Avg loss in epoch(incomplete): 0.15965911746\n",
      "Epoch: 1 global_step 8725 i 29 Avg loss in epoch(incomplete): 0.160031673312\n",
      "saving\n",
      "Epoch: 1 global_step 8750 i 54 Avg loss in epoch(incomplete): 0.160448163748\n",
      "Epoch: 1 global_step 8750 i 54 Avg loss in epoch(incomplete): 0.160448163748\n",
      "Epoch: 1 global_step 8775 i 79 Avg loss in epoch(incomplete): 0.160231954977\n",
      "Epoch: 1 global_step 8800 i 104 Avg loss in epoch(incomplete): 0.160028758787\n",
      "Epoch: 1 global_step 8825 i 129 Avg loss in epoch(incomplete): 0.159688501977\n",
      "Epoch: 1 global_step 8850 i 154 Avg loss in epoch(incomplete): 0.159754593911\n",
      "Epoch: 1 global_step 8875 i 179 Avg loss in epoch(incomplete): 0.159869195935\n",
      "Epoch: 1 global_step 8900 i 204 Avg loss in epoch(incomplete): 0.15983577218\n",
      "Epoch: 1 global_step 8925 i 229 Avg loss in epoch(incomplete): 0.160060387137\n",
      "Epoch: 1 global_step 8950 i 254 Avg loss in epoch(incomplete): 0.15990531877\n",
      "Epoch: 1 global_step 8975 i 279 Avg loss in epoch(incomplete): 0.159753491996\n",
      "saving\n",
      "Epoch: 1 global_step 9000 i 304 Avg loss in epoch(incomplete): 0.159742948217\n",
      "Epoch: 1 global_step 9000 i 304 Avg loss in epoch(incomplete): 0.159742948217\n",
      "Epoch: 1 global_step 9025 i 329 Avg loss in epoch(incomplete): 0.159890721874\n",
      "Epoch: 1 global_step 9050 i 354 Avg loss in epoch(incomplete): 0.160007222647\n",
      "Epoch: 1 global_step 9075 i 379 Avg loss in epoch(incomplete): 0.160071719869\n",
      "Epoch: 1 global_step 9100 i 404 Avg loss in epoch(incomplete): 0.159996727183\n",
      "Epoch: 1 global_step 9125 i 429 Avg loss in epoch(incomplete): 0.159984511761\n",
      "Epoch: 1 global_step 9150 i 454 Avg loss in epoch(incomplete): 0.159968327526\n",
      "Epoch: 1 global_step 9175 i 479 Avg loss in epoch(incomplete): 0.159953255082\n",
      "Epoch: 1 global_step 9200 i 504 Avg loss in epoch(incomplete): 0.160008541133\n",
      "Epoch: 1 global_step 9225 i 529 Avg loss in epoch(incomplete): 0.160036379914\n",
      "saving\n",
      "Epoch: 1 global_step 9250 i 554 Avg loss in epoch(incomplete): 0.159991518012\n",
      "Epoch: 1 global_step 9250 i 554 Avg loss in epoch(incomplete): 0.159991518012\n",
      "Epoch: 1 global_step 9275 i 579 Avg loss in epoch(incomplete): 0.159964135315\n",
      "Epoch: 1 global_step 9300 i 604 Avg loss in epoch(incomplete): 0.160015453903\n",
      "Epoch: 1 global_step 9325 i 629 Avg loss in epoch(incomplete): 0.159999852308\n",
      "Epoch: 1 global_step 9350 i 654 Avg loss in epoch(incomplete): 0.159961161336\n",
      "Epoch: 1 global_step 9375 i 679 Avg loss in epoch(incomplete): 0.159984866289\n",
      "Epoch: 1 global_step 9400 i 704 Avg loss in epoch(incomplete): 0.160009782872\n",
      "Epoch: 1 global_step 9425 i 729 Avg loss in epoch(incomplete): 0.160035549574\n",
      "Epoch: 1 global_step 9450 i 754 Avg loss in epoch(incomplete): 0.160015049774\n",
      "Epoch: 1 global_step 9475 i 779 Avg loss in epoch(incomplete): 0.160010665349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "Epoch: 1 global_step 9500 i 804 Avg loss in epoch(incomplete): 0.160008446641\n",
      "Epoch: 1 global_step 9500 i 804 Avg loss in epoch(incomplete): 0.160008446641\n",
      "Epoch: 1 global_step 9525 i 829 Avg loss in epoch(incomplete): 0.160012243347\n",
      "Epoch: 1 global_step 9550 i 854 Avg loss in epoch(incomplete): 0.160013900462\n",
      "Epoch: 1 global_step 9575 i 879 Avg loss in epoch(incomplete): 0.159993293407\n",
      "Epoch: 1 global_step 9600 i 904 Avg loss in epoch(incomplete): 0.160013082969\n",
      "Epoch: 1 global_step 9625 i 929 Avg loss in epoch(incomplete): 0.159995003846\n",
      "Epoch: 1 global_step 9650 i 954 Avg loss in epoch(incomplete): 0.159997042031\n",
      "Epoch: 1 global_step 9675 i 979 Avg loss in epoch(incomplete): 0.159999943585\n",
      "Epoch: 1 global_step 9700 i 1004 Avg loss in epoch(incomplete): 0.159991382337\n",
      "Epoch: 1 global_step 9725 i 1029 Avg loss in epoch(incomplete): 0.159999919733\n",
      "saving\n",
      "Epoch: 1 global_step 9750 i 1054 Avg loss in epoch(incomplete): 0.159985014147\n",
      "Epoch: 1 global_step 9750 i 1054 Avg loss in epoch(incomplete): 0.159985014147\n",
      "Epoch: 1 global_step 9775 i 1079 Avg loss in epoch(incomplete): 0.159965734267\n",
      "Epoch: 1 global_step 9800 i 1104 Avg loss in epoch(incomplete): 0.159948393878\n",
      "Epoch: 1 global_step 9825 i 1129 Avg loss in epoch(incomplete): 0.159919681895\n",
      "Epoch: 1 global_step 9850 i 1154 Avg loss in epoch(incomplete): 0.159889922591\n",
      "Epoch: 1 global_step 9875 i 1179 Avg loss in epoch(incomplete): 0.159886583092\n",
      "Epoch: 1 global_step 9900 i 1204 Avg loss in epoch(incomplete): 0.159874715486\n",
      "Epoch: 1 global_step 9925 i 1229 Avg loss in epoch(incomplete): 0.159879674199\n",
      "Epoch: 1 global_step 9950 i 1254 Avg loss in epoch(incomplete): 0.159915404135\n",
      "Epoch: 1 global_step 9975 i 1279 Avg loss in epoch(incomplete): 0.15991292228\n",
      "saving\n",
      "Epoch: 1 global_step 10000 i 1304 Avg loss in epoch(incomplete): 0.159895627355\n",
      "Epoch: 1 global_step 10000 i 1304 Avg loss in epoch(incomplete): 0.159895627355\n",
      "Epoch: 1 global_step 10025 i 1329 Avg loss in epoch(incomplete): 0.159907435138\n",
      "Epoch: 1 global_step 10050 i 1354 Avg loss in epoch(incomplete): 0.159889278179\n",
      "Epoch: 1 global_step 10075 i 1379 Avg loss in epoch(incomplete): 0.159900672323\n",
      "Epoch: 1 global_step 10100 i 1404 Avg loss in epoch(incomplete): 0.159924394136\n",
      "Epoch: 1 global_step 10125 i 1429 Avg loss in epoch(incomplete): 0.159915865364\n",
      "Epoch: 1 global_step 10150 i 1454 Avg loss in epoch(incomplete): 0.159914386406\n",
      "Epoch: 1 global_step 10175 i 1479 Avg loss in epoch(incomplete): 0.15989751846\n",
      "Epoch: 1 global_step 10200 i 1504 Avg loss in epoch(incomplete): 0.159912176831\n",
      "Epoch: 1 global_step 10225 i 1529 Avg loss in epoch(incomplete): 0.159914557129\n",
      "saving\n",
      "Epoch: 1 global_step 10250 i 1554 Avg loss in epoch(incomplete): 0.159912500849\n",
      "Epoch: 1 global_step 10250 i 1554 Avg loss in epoch(incomplete): 0.159912500849\n",
      "Epoch: 1 global_step 10275 i 1579 Avg loss in epoch(incomplete): 0.159925526701\n",
      "Epoch: 1 global_step 10300 i 1604 Avg loss in epoch(incomplete): 0.159913353543\n",
      "Epoch: 1 global_step 10325 i 1629 Avg loss in epoch(incomplete): 0.159904833504\n",
      "Epoch: 1 global_step 10350 i 1654 Avg loss in epoch(incomplete): 0.159911538305\n",
      "Epoch: 1 global_step 10375 i 1679 Avg loss in epoch(incomplete): 0.159927040072\n",
      "Epoch: 1 global_step 10400 i 1704 Avg loss in epoch(incomplete): 0.159925689052\n",
      "Epoch: 1 global_step 10425 i 1729 Avg loss in epoch(incomplete): 0.159893942732\n",
      "Epoch: 1 global_step 10450 i 1754 Avg loss in epoch(incomplete): 0.15990258564\n",
      "Epoch: 1 global_step 10475 i 1779 Avg loss in epoch(incomplete): 0.159919795838\n",
      "saving\n",
      "Epoch: 1 global_step 10500 i 1804 Avg loss in epoch(incomplete): 0.159917636922\n",
      "Epoch: 1 global_step 10500 i 1804 Avg loss in epoch(incomplete): 0.159917636922\n",
      "Epoch: 1 global_step 10525 i 1829 Avg loss in epoch(incomplete): 0.159927595134\n",
      "Epoch: 1 global_step 10550 i 1854 Avg loss in epoch(incomplete): 0.159927012408\n",
      "Epoch: 1 global_step 10575 i 1879 Avg loss in epoch(incomplete): 0.159916815384\n",
      "Epoch: 1 global_step 10600 i 1904 Avg loss in epoch(incomplete): 0.159920765886\n",
      "Epoch: 1 global_step 10625 i 1929 Avg loss in epoch(incomplete): 0.159930551847\n",
      "Epoch: 1 global_step 10650 i 1954 Avg loss in epoch(incomplete): 0.159919836774\n",
      "Epoch: 1 global_step 10675 i 1979 Avg loss in epoch(incomplete): 0.159889371768\n",
      "Epoch: 1 global_step 10700 i 2004 Avg loss in epoch(incomplete): 0.159880556987\n",
      "Epoch: 1 global_step 10725 i 2029 Avg loss in epoch(incomplete): 0.15988215592\n",
      "saving\n",
      "Epoch: 1 global_step 10750 i 2054 Avg loss in epoch(incomplete): 0.15987791237\n",
      "Epoch: 1 global_step 10750 i 2054 Avg loss in epoch(incomplete): 0.15987791237\n",
      "Epoch: 1 global_step 10775 i 2079 Avg loss in epoch(incomplete): 0.159884360726\n",
      "Epoch: 1 global_step 10800 i 2104 Avg loss in epoch(incomplete): 0.159889262363\n",
      "Epoch: 1 global_step 10825 i 2129 Avg loss in epoch(incomplete): 0.159880303126\n",
      "Epoch: 1 global_step 10850 i 2154 Avg loss in epoch(incomplete): 0.15987207559\n",
      "Epoch: 1 global_step 10875 i 2179 Avg loss in epoch(incomplete): 0.159873581855\n",
      "Epoch: 1 global_step 10900 i 2204 Avg loss in epoch(incomplete): 0.159870761092\n",
      "Epoch: 1 global_step 10925 i 2229 Avg loss in epoch(incomplete): 0.159874366277\n",
      "Epoch: 1 global_step 10950 i 2254 Avg loss in epoch(incomplete): 0.15987765795\n",
      "Epoch: 1 global_step 10975 i 2279 Avg loss in epoch(incomplete): 0.159895046585\n",
      "saving\n",
      "Epoch: 1 global_step 11000 i 2304 Avg loss in epoch(incomplete): 0.159888100702\n",
      "Epoch: 1 global_step 11000 i 2304 Avg loss in epoch(incomplete): 0.159888100702\n",
      "Epoch: 1 global_step 11025 i 2329 Avg loss in epoch(incomplete): 0.159871581382\n",
      "Epoch: 1 global_step 11050 i 2354 Avg loss in epoch(incomplete): 0.159865972235\n",
      "Epoch: 1 global_step 11075 i 2379 Avg loss in epoch(incomplete): 0.159865138072\n",
      "Epoch: 1 global_step 11100 i 2404 Avg loss in epoch(incomplete): 0.159857894149\n",
      "Epoch: 1 global_step 11125 i 2429 Avg loss in epoch(incomplete): 0.159831767036\n",
      "Epoch: 1 global_step 11150 i 2454 Avg loss in epoch(incomplete): 0.159837833781\n",
      "Epoch: 1 global_step 11175 i 2479 Avg loss in epoch(incomplete): 0.159833118728\n",
      "Epoch: 1 global_step 11200 i 2504 Avg loss in epoch(incomplete): 0.159819524879\n",
      "Epoch: 1 global_step 11225 i 2529 Avg loss in epoch(incomplete): 0.159811742539\n",
      "saving\n",
      "Epoch: 1 global_step 11250 i 2554 Avg loss in epoch(incomplete): 0.159792383298\n",
      "Epoch: 1 global_step 11250 i 2554 Avg loss in epoch(incomplete): 0.159792383298\n",
      "Epoch: 1 global_step 11275 i 2579 Avg loss in epoch(incomplete): 0.159796017922\n",
      "Epoch: 1 global_step 11300 i 2604 Avg loss in epoch(incomplete): 0.159803255397\n",
      "Epoch: 1 global_step 11325 i 2629 Avg loss in epoch(incomplete): 0.159810222515\n",
      "Epoch: 1 global_step 11350 i 2654 Avg loss in epoch(incomplete): 0.159806284971\n",
      "Epoch: 1 global_step 11375 i 2679 Avg loss in epoch(incomplete): 0.159810670115\n",
      "Epoch: 1 global_step 11400 i 2704 Avg loss in epoch(incomplete): 0.15981157768\n",
      "Epoch: 1 global_step 11425 i 2729 Avg loss in epoch(incomplete): 0.159818003903\n",
      "Epoch: 1 global_step 11450 i 2754 Avg loss in epoch(incomplete): 0.159808961833\n",
      "Epoch: 1 global_step 11475 i 2779 Avg loss in epoch(incomplete): 0.159805764263\n",
      "saving\n",
      "Epoch: 1 global_step 11500 i 2804 Avg loss in epoch(incomplete): 0.159793042818\n",
      "Epoch: 1 global_step 11500 i 2804 Avg loss in epoch(incomplete): 0.159793042818\n",
      "Epoch: 1 global_step 11525 i 2829 Avg loss in epoch(incomplete): 0.15978127807\n",
      "Epoch: 1 global_step 11550 i 2854 Avg loss in epoch(incomplete): 0.159775284611\n",
      "Epoch: 1 global_step 11575 i 2879 Avg loss in epoch(incomplete): 0.159774173408\n",
      "Epoch: 1 global_step 11600 i 2904 Avg loss in epoch(incomplete): 0.159765809605\n",
      "Epoch: 1 global_step 11625 i 2929 Avg loss in epoch(incomplete): 0.15975477666\n",
      "Epoch: 1 global_step 11650 i 2954 Avg loss in epoch(incomplete): 0.159747234204\n",
      "Epoch: 1 global_step 11675 i 2979 Avg loss in epoch(incomplete): 0.159741215338\n",
      "Epoch: 1 global_step 11700 i 3004 Avg loss in epoch(incomplete): 0.159737219013\n",
      "Epoch: 1 global_step 11725 i 3029 Avg loss in epoch(incomplete): 0.159744114657\n",
      "saving\n",
      "Epoch: 1 global_step 11750 i 3054 Avg loss in epoch(incomplete): 0.15974081941\n",
      "Epoch: 1 global_step 11750 i 3054 Avg loss in epoch(incomplete): 0.15974081941\n",
      "Epoch: 1 global_step 11775 i 3079 Avg loss in epoch(incomplete): 0.159736295712\n",
      "Epoch: 1 global_step 11800 i 3104 Avg loss in epoch(incomplete): 0.159733776573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 global_step 11825 i 3129 Avg loss in epoch(incomplete): 0.159730889427\n",
      "Epoch: 1 global_step 11850 i 3154 Avg loss in epoch(incomplete): 0.159720011799\n",
      "Epoch: 1 global_step 11875 i 3179 Avg loss in epoch(incomplete): 0.159722820496\n",
      "Epoch: 1 global_step 11900 i 3204 Avg loss in epoch(incomplete): 0.159723386224\n",
      "Epoch: 1 global_step 11925 i 3229 Avg loss in epoch(incomplete): 0.159723421839\n",
      "Epoch: 1 global_step 11950 i 3254 Avg loss in epoch(incomplete): 0.159716162589\n",
      "Epoch: 1 global_step 11975 i 3279 Avg loss in epoch(incomplete): 0.159717578673\n",
      "saving\n",
      "Epoch: 1 global_step 12000 i 3304 Avg loss in epoch(incomplete): 0.159708403995\n",
      "Epoch: 1 global_step 12000 i 3304 Avg loss in epoch(incomplete): 0.159708403995\n",
      "Epoch: 1 global_step 12025 i 3329 Avg loss in epoch(incomplete): 0.15970824044\n",
      "Epoch: 1 global_step 12050 i 3354 Avg loss in epoch(incomplete): 0.159695692674\n",
      "Epoch: 1 global_step 12075 i 3379 Avg loss in epoch(incomplete): 0.159683679855\n",
      "Epoch: 1 global_step 12100 i 3404 Avg loss in epoch(incomplete): 0.159670871203\n",
      "Epoch: 1 global_step 12125 i 3429 Avg loss in epoch(incomplete): 0.159665415873\n",
      "Epoch: 1 global_step 12150 i 3454 Avg loss in epoch(incomplete): 0.159660950407\n",
      "Epoch: 1 global_step 12175 i 3479 Avg loss in epoch(incomplete): 0.159651768285\n",
      "Epoch: 1 global_step 12200 i 3504 Avg loss in epoch(incomplete): 0.159648170098\n",
      "Epoch: 1 global_step 12225 i 3529 Avg loss in epoch(incomplete): 0.159647751733\n",
      "saving\n",
      "Epoch: 1 global_step 12250 i 3554 Avg loss in epoch(incomplete): 0.159639902381\n",
      "Epoch: 1 global_step 12250 i 3554 Avg loss in epoch(incomplete): 0.159639902381\n",
      "Epoch: 1 global_step 12275 i 3579 Avg loss in epoch(incomplete): 0.159629899101\n",
      "Epoch: 1 global_step 12300 i 3604 Avg loss in epoch(incomplete): 0.159633550999\n",
      "Epoch: 1 global_step 12325 i 3629 Avg loss in epoch(incomplete): 0.15963737346\n",
      "Epoch: 1 global_step 12350 i 3654 Avg loss in epoch(incomplete): 0.159634554635\n",
      "Epoch: 1 global_step 12375 i 3679 Avg loss in epoch(incomplete): 0.159629540267\n",
      "Epoch: 1 global_step 12400 i 3704 Avg loss in epoch(incomplete): 0.159621436542\n",
      "Epoch: 1 global_step 12425 i 3729 Avg loss in epoch(incomplete): 0.159621065444\n",
      "Epoch: 1 global_step 12450 i 3754 Avg loss in epoch(incomplete): 0.159623408242\n",
      "Epoch: 1 global_step 12475 i 3779 Avg loss in epoch(incomplete): 0.159614611255\n",
      "saving\n",
      "Epoch: 1 global_step 12500 i 3804 Avg loss in epoch(incomplete): 0.159621522539\n",
      "Epoch: 1 global_step 12500 i 3804 Avg loss in epoch(incomplete): 0.159621522539\n",
      "Epoch: 1 global_step 12525 i 3829 Avg loss in epoch(incomplete): 0.159624360101\n",
      "Epoch: 1 global_step 12550 i 3854 Avg loss in epoch(incomplete): 0.159621801296\n",
      "Epoch: 1 global_step 12575 i 3879 Avg loss in epoch(incomplete): 0.159623091445\n",
      "Epoch: 1 global_step 12600 i 3904 Avg loss in epoch(incomplete): 0.159617467486\n",
      "Epoch: 1 global_step 12625 i 3929 Avg loss in epoch(incomplete): 0.159607820418\n",
      "Epoch: 1 global_step 12650 i 3954 Avg loss in epoch(incomplete): 0.15960710179\n",
      "Epoch: 1 global_step 12675 i 3979 Avg loss in epoch(incomplete): 0.159604810961\n",
      "Epoch: 1 global_step 12700 i 4004 Avg loss in epoch(incomplete): 0.159598125873\n",
      "Epoch: 1 global_step 12725 i 4029 Avg loss in epoch(incomplete): 0.159591291571\n",
      "saving\n",
      "Epoch: 1 global_step 12750 i 4054 Avg loss in epoch(incomplete): 0.159587476947\n",
      "Epoch: 1 global_step 12750 i 4054 Avg loss in epoch(incomplete): 0.159587476947\n",
      "Epoch: 1 global_step 12775 i 4079 Avg loss in epoch(incomplete): 0.159584640175\n",
      "Epoch: 1 global_step 12800 i 4104 Avg loss in epoch(incomplete): 0.15957292624\n",
      "Epoch: 1 global_step 12825 i 4129 Avg loss in epoch(incomplete): 0.159566714739\n",
      "Epoch: 1 global_step 12850 i 4154 Avg loss in epoch(incomplete): 0.159561723309\n",
      "Epoch: 1 global_step 12875 i 4179 Avg loss in epoch(incomplete): 0.159559634588\n",
      "Epoch: 1 global_step 12900 i 4204 Avg loss in epoch(incomplete): 0.159552061845\n",
      "Epoch: 1 global_step 12925 i 4229 Avg loss in epoch(incomplete): 0.159550370163\n",
      "Epoch: 1 global_step 12950 i 4254 Avg loss in epoch(incomplete): 0.159540786008\n",
      "Epoch: 1 global_step 12975 i 4279 Avg loss in epoch(incomplete): 0.159536719824\n",
      "saving\n",
      "Epoch: 1 global_step 13000 i 4304 Avg loss in epoch(incomplete): 0.159534458395\n",
      "Epoch: 1 global_step 13000 i 4304 Avg loss in epoch(incomplete): 0.159534458395\n",
      "Epoch: 1 global_step 13025 i 4329 Avg loss in epoch(incomplete): 0.159530721168\n",
      "Epoch: 1 global_step 13050 i 4354 Avg loss in epoch(incomplete): 0.159528380574\n",
      "Epoch: 1 global_step 13075 i 4379 Avg loss in epoch(incomplete): 0.159535799689\n",
      "Epoch: 1 global_step 13100 i 4404 Avg loss in epoch(incomplete): 0.159530806169\n",
      "Epoch: 1 global_step 13125 i 4429 Avg loss in epoch(incomplete): 0.159524543822\n",
      "Epoch: 1 global_step 13150 i 4454 Avg loss in epoch(incomplete): 0.15951225958\n",
      "Epoch: 1 global_step 13175 i 4479 Avg loss in epoch(incomplete): 0.159507708624\n",
      "Epoch: 1 global_step 13200 i 4504 Avg loss in epoch(incomplete): 0.159508123187\n",
      "Epoch: 1 global_step 13225 i 4529 Avg loss in epoch(incomplete): 0.159509732622\n",
      "saving\n",
      "Epoch: 1 global_step 13250 i 4554 Avg loss in epoch(incomplete): 0.159504502986\n",
      "Epoch: 1 global_step 13250 i 4554 Avg loss in epoch(incomplete): 0.159504502986\n",
      "Epoch: 1 global_step 13275 i 4579 Avg loss in epoch(incomplete): 0.159502158571\n",
      "Epoch: 1 global_step 13300 i 4604 Avg loss in epoch(incomplete): 0.159490966477\n",
      "Epoch: 1 global_step 13325 i 4629 Avg loss in epoch(incomplete): 0.159484621538\n",
      "Epoch: 1 global_step 13350 i 4654 Avg loss in epoch(incomplete): 0.159482874807\n",
      "Epoch: 1 global_step 13375 i 4679 Avg loss in epoch(incomplete): 0.159484468123\n",
      "Epoch: 1 global_step 13400 i 4704 Avg loss in epoch(incomplete): 0.159484541289\n",
      "Epoch: 1 global_step 13425 i 4729 Avg loss in epoch(incomplete): 0.159475726951\n",
      "Epoch: 1 global_step 13450 i 4754 Avg loss in epoch(incomplete): 0.159474877645\n",
      "Epoch: 1 global_step 13475 i 4779 Avg loss in epoch(incomplete): 0.15947991223\n",
      "saving\n",
      "Epoch: 1 global_step 13500 i 4804 Avg loss in epoch(incomplete): 0.159472222749\n",
      "Epoch: 1 global_step 13500 i 4804 Avg loss in epoch(incomplete): 0.159472222749\n",
      "Epoch: 1 global_step 13525 i 4829 Avg loss in epoch(incomplete): 0.159463588866\n",
      "Epoch: 1 global_step 13550 i 4854 Avg loss in epoch(incomplete): 0.159462337339\n",
      "Epoch: 1 global_step 13575 i 4879 Avg loss in epoch(incomplete): 0.159463269951\n",
      "Epoch: 1 global_step 13600 i 4904 Avg loss in epoch(incomplete): 0.159466355933\n",
      "Epoch: 1 global_step 13625 i 4929 Avg loss in epoch(incomplete): 0.15946482189\n",
      "Epoch: 1 global_step 13650 i 4954 Avg loss in epoch(incomplete): 0.15945838258\n",
      "Epoch: 1 global_step 13675 i 4979 Avg loss in epoch(incomplete): 0.159458802263\n",
      "Epoch: 1 global_step 13700 i 5004 Avg loss in epoch(incomplete): 0.159450507235\n",
      "Epoch: 1 global_step 13725 i 5029 Avg loss in epoch(incomplete): 0.159442982119\n",
      "saving\n",
      "Epoch: 1 global_step 13750 i 5054 Avg loss in epoch(incomplete): 0.159430508037\n",
      "Epoch: 1 global_step 13750 i 5054 Avg loss in epoch(incomplete): 0.159430508037\n",
      "Epoch: 1 global_step 13775 i 5079 Avg loss in epoch(incomplete): 0.159423993164\n",
      "Epoch: 1 global_step 13800 i 5104 Avg loss in epoch(incomplete): 0.159421288208\n",
      "Epoch: 1 global_step 13825 i 5129 Avg loss in epoch(incomplete): 0.159423213047\n",
      "Epoch: 1 global_step 13850 i 5154 Avg loss in epoch(incomplete): 0.159427861975\n",
      "Epoch: 1 global_step 13875 i 5179 Avg loss in epoch(incomplete): 0.159420363139\n",
      "Epoch: 1 global_step 13900 i 5204 Avg loss in epoch(incomplete): 0.159425356678\n",
      "Epoch: 1 global_step 13925 i 5229 Avg loss in epoch(incomplete): 0.159426561343\n",
      "Epoch: 1 global_step 13950 i 5254 Avg loss in epoch(incomplete): 0.159424939077\n",
      "Epoch: 1 global_step 13975 i 5279 Avg loss in epoch(incomplete): 0.159419249783\n",
      "saving\n",
      "Epoch: 1 global_step 14000 i 5304 Avg loss in epoch(incomplete): 0.159423840099\n",
      "Epoch: 1 global_step 14000 i 5304 Avg loss in epoch(incomplete): 0.159423840099\n",
      "Epoch: 1 global_step 14025 i 5329 Avg loss in epoch(incomplete): 0.159425044183\n",
      "Epoch: 1 global_step 14050 i 5354 Avg loss in epoch(incomplete): 0.159415685936\n",
      "Epoch: 1 global_step 14075 i 5379 Avg loss in epoch(incomplete): 0.159413478044\n",
      "Epoch: 1 global_step 14100 i 5404 Avg loss in epoch(incomplete): 0.159399840944\n",
      "Epoch: 1 global_step 14125 i 5429 Avg loss in epoch(incomplete): 0.159396012981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 global_step 14150 i 5454 Avg loss in epoch(incomplete): 0.159394189745\n",
      "Epoch: 1 global_step 14175 i 5479 Avg loss in epoch(incomplete): 0.159388879659\n",
      "Epoch: 1 global_step 14200 i 5504 Avg loss in epoch(incomplete): 0.159379664421\n",
      "Epoch: 1 global_step 14225 i 5529 Avg loss in epoch(incomplete): 0.15937803936\n",
      "saving\n",
      "Epoch: 1 global_step 14250 i 5554 Avg loss in epoch(incomplete): 0.159373467536\n",
      "Epoch: 1 global_step 14250 i 5554 Avg loss in epoch(incomplete): 0.159373467536\n",
      "Epoch: 1 global_step 14275 i 5579 Avg loss in epoch(incomplete): 0.159370219045\n",
      "Epoch: 1 global_step 14300 i 5604 Avg loss in epoch(incomplete): 0.15936077038\n",
      "Epoch: 1 global_step 14325 i 5629 Avg loss in epoch(incomplete): 0.159362253626\n",
      "Epoch: 1 global_step 14350 i 5654 Avg loss in epoch(incomplete): 0.159356063011\n",
      "Epoch: 1 global_step 14375 i 5679 Avg loss in epoch(incomplete): 0.159356175447\n",
      "Epoch: 1 global_step 14400 i 5704 Avg loss in epoch(incomplete): 0.159354413444\n",
      "Epoch: 1 global_step 14425 i 5729 Avg loss in epoch(incomplete): 0.159349865543\n",
      "Epoch: 1 global_step 14450 i 5754 Avg loss in epoch(incomplete): 0.159346570363\n",
      "Epoch: 1 global_step 14475 i 5779 Avg loss in epoch(incomplete): 0.159341179159\n",
      "saving\n",
      "Epoch: 1 global_step 14500 i 5804 Avg loss in epoch(incomplete): 0.159342458718\n",
      "Epoch: 1 global_step 14500 i 5804 Avg loss in epoch(incomplete): 0.159342458718\n",
      "Epoch: 1 global_step 14525 i 5829 Avg loss in epoch(incomplete): 0.159336932719\n",
      "Epoch: 1 global_step 14550 i 5854 Avg loss in epoch(incomplete): 0.159334465505\n",
      "Epoch: 1 global_step 14575 i 5879 Avg loss in epoch(incomplete): 0.159328104698\n",
      "Epoch: 1 global_step 14600 i 5904 Avg loss in epoch(incomplete): 0.159327465811\n",
      "Epoch: 1 global_step 14625 i 5929 Avg loss in epoch(incomplete): 0.159325537684\n",
      "Epoch: 1 global_step 14650 i 5954 Avg loss in epoch(incomplete): 0.159314841948\n",
      "Epoch: 1 global_step 14675 i 5979 Avg loss in epoch(incomplete): 0.159311964906\n",
      "Epoch: 1 global_step 14700 i 6004 Avg loss in epoch(incomplete): 0.159311149373\n",
      "Epoch: 1 global_step 14725 i 6029 Avg loss in epoch(incomplete): 0.159311873835\n",
      "saving\n",
      "Epoch: 1 global_step 14750 i 6054 Avg loss in epoch(incomplete): 0.159306687449\n",
      "Epoch: 1 global_step 14750 i 6054 Avg loss in epoch(incomplete): 0.159306687449\n",
      "Epoch: 1 global_step 14775 i 6079 Avg loss in epoch(incomplete): 0.159295467023\n",
      "Epoch: 1 global_step 14800 i 6104 Avg loss in epoch(incomplete): 0.159293212523\n",
      "Epoch: 1 global_step 14825 i 6129 Avg loss in epoch(incomplete): 0.159287884858\n",
      "Epoch: 1 global_step 14850 i 6154 Avg loss in epoch(incomplete): 0.159286702933\n",
      "Epoch: 1 global_step 14875 i 6179 Avg loss in epoch(incomplete): 0.159283670166\n",
      "Epoch: 1 global_step 14900 i 6204 Avg loss in epoch(incomplete): 0.159275579006\n",
      "Epoch: 1 global_step 14925 i 6229 Avg loss in epoch(incomplete): 0.159275314695\n",
      "Epoch: 1 global_step 14950 i 6254 Avg loss in epoch(incomplete): 0.159269919232\n",
      "Epoch: 1 global_step 14975 i 6279 Avg loss in epoch(incomplete): 0.159263529095\n",
      "saving\n",
      "Epoch: 1 global_step 15000 i 6304 Avg loss in epoch(incomplete): 0.159257265453\n",
      "Epoch: 1 global_step 15000 i 6304 Avg loss in epoch(incomplete): 0.159257265453\n",
      "Epoch: 1 global_step 15025 i 6329 Avg loss in epoch(incomplete): 0.159256626866\n",
      "Epoch: 1 global_step 15050 i 6354 Avg loss in epoch(incomplete): 0.15925296882\n",
      "Epoch: 1 global_step 15075 i 6379 Avg loss in epoch(incomplete): 0.159247032714\n",
      "Epoch: 1 global_step 15100 i 6404 Avg loss in epoch(incomplete): 0.159245817097\n",
      "Epoch: 1 global_step 15125 i 6429 Avg loss in epoch(incomplete): 0.159244282768\n",
      "Epoch: 1 global_step 15150 i 6454 Avg loss in epoch(incomplete): 0.159237540029\n",
      "Epoch: 1 global_step 15175 i 6479 Avg loss in epoch(incomplete): 0.159233539439\n",
      "Epoch: 1 global_step 15200 i 6504 Avg loss in epoch(incomplete): 0.159236671728\n",
      "Epoch: 1 global_step 15225 i 6529 Avg loss in epoch(incomplete): 0.159239761467\n",
      "saving\n",
      "Epoch: 1 global_step 15250 i 6554 Avg loss in epoch(incomplete): 0.159234490338\n",
      "Epoch: 1 global_step 15250 i 6554 Avg loss in epoch(incomplete): 0.159234490338\n",
      "Epoch: 1 global_step 15275 i 6579 Avg loss in epoch(incomplete): 0.15923199037\n",
      "Epoch: 1 global_step 15300 i 6604 Avg loss in epoch(incomplete): 0.15922981019\n",
      "Epoch: 1 global_step 15325 i 6629 Avg loss in epoch(incomplete): 0.159229445264\n",
      "Epoch: 1 global_step 15350 i 6654 Avg loss in epoch(incomplete): 0.159226588937\n",
      "Epoch: 1 global_step 15375 i 6679 Avg loss in epoch(incomplete): 0.159222473093\n",
      "Epoch: 1 global_step 15400 i 6704 Avg loss in epoch(incomplete): 0.159224215308\n",
      "Epoch: 1 global_step 15425 i 6729 Avg loss in epoch(incomplete): 0.15922027879\n",
      "Epoch: 1 global_step 15450 i 6754 Avg loss in epoch(incomplete): 0.159214302986\n",
      "Epoch: 1 global_step 15475 i 6779 Avg loss in epoch(incomplete): 0.159216329996\n",
      "saving\n",
      "Epoch: 1 global_step 15500 i 6804 Avg loss in epoch(incomplete): 0.159213835947\n",
      "Epoch: 1 global_step 15500 i 6804 Avg loss in epoch(incomplete): 0.159213835947\n",
      "Epoch: 1 global_step 15525 i 6829 Avg loss in epoch(incomplete): 0.159212540169\n",
      "Epoch: 1 global_step 15550 i 6854 Avg loss in epoch(incomplete): 0.159207220194\n",
      "Epoch: 1 global_step 15575 i 6879 Avg loss in epoch(incomplete): 0.159203654707\n",
      "Epoch: 1 global_step 15600 i 6904 Avg loss in epoch(incomplete): 0.159203803837\n",
      "Epoch: 1 global_step 15625 i 6929 Avg loss in epoch(incomplete): 0.159200559337\n",
      "Epoch: 1 global_step 15650 i 6954 Avg loss in epoch(incomplete): 0.159196175186\n",
      "Epoch: 1 global_step 15675 i 6979 Avg loss in epoch(incomplete): 0.159193186338\n",
      "Epoch: 1 global_step 15700 i 7004 Avg loss in epoch(incomplete): 0.159191416941\n",
      "Epoch: 1 global_step 15725 i 7029 Avg loss in epoch(incomplete): 0.159182369713\n",
      "saving\n",
      "Epoch: 1 global_step 15750 i 7054 Avg loss in epoch(incomplete): 0.159187313008\n",
      "Epoch: 1 global_step 15750 i 7054 Avg loss in epoch(incomplete): 0.159187313008\n",
      "Epoch: 1 global_step 15775 i 7079 Avg loss in epoch(incomplete): 0.159182708864\n",
      "Epoch: 1 global_step 15800 i 7104 Avg loss in epoch(incomplete): 0.159180866221\n",
      "Epoch: 1 global_step 15825 i 7129 Avg loss in epoch(incomplete): 0.159177097902\n",
      "Epoch: 1 global_step 15850 i 7154 Avg loss in epoch(incomplete): 0.159178066464\n",
      "Epoch: 1 global_step 15875 i 7179 Avg loss in epoch(incomplete): 0.159180711083\n",
      "Epoch: 1 global_step 15900 i 7204 Avg loss in epoch(incomplete): 0.159176449797\n",
      "Epoch: 1 global_step 15925 i 7229 Avg loss in epoch(incomplete): 0.159170408122\n",
      "Epoch: 1 global_step 15950 i 7254 Avg loss in epoch(incomplete): 0.159168041475\n",
      "Epoch: 1 global_step 15975 i 7279 Avg loss in epoch(incomplete): 0.159164968346\n",
      "saving\n",
      "Epoch: 1 global_step 16000 i 7304 Avg loss in epoch(incomplete): 0.159163624322\n",
      "Epoch: 1 global_step 16000 i 7304 Avg loss in epoch(incomplete): 0.159163624322\n",
      "Epoch: 1 global_step 16025 i 7329 Avg loss in epoch(incomplete): 0.159161535442\n",
      "Epoch: 1 global_step 16050 i 7354 Avg loss in epoch(incomplete): 0.159152834228\n",
      "Epoch: 1 global_step 16075 i 7379 Avg loss in epoch(incomplete): 0.159150355214\n",
      "Epoch: 1 global_step 16100 i 7404 Avg loss in epoch(incomplete): 0.159142459465\n",
      "Epoch: 1 global_step 16125 i 7429 Avg loss in epoch(incomplete): 0.159137664743\n",
      "Epoch: 1 global_step 16150 i 7454 Avg loss in epoch(incomplete): 0.15913692704\n",
      "Epoch: 1 global_step 16175 i 7479 Avg loss in epoch(incomplete): 0.159136041758\n",
      "Epoch: 1 global_step 16200 i 7504 Avg loss in epoch(incomplete): 0.159132183325\n",
      "Epoch: 1 global_step 16225 i 7529 Avg loss in epoch(incomplete): 0.159130895241\n",
      "saving\n",
      "Epoch: 1 global_step 16250 i 7554 Avg loss in epoch(incomplete): 0.159129605991\n",
      "Epoch: 1 global_step 16250 i 7554 Avg loss in epoch(incomplete): 0.159129605991\n",
      "Epoch: 1 global_step 16275 i 7579 Avg loss in epoch(incomplete): 0.159130568566\n",
      "Epoch: 1 global_step 16300 i 7604 Avg loss in epoch(incomplete): 0.159128330167\n",
      "Epoch: 1 global_step 16325 i 7629 Avg loss in epoch(incomplete): 0.159122132937\n",
      "Epoch: 1 global_step 16350 i 7654 Avg loss in epoch(incomplete): 0.159122460689\n",
      "Epoch: 1 global_step 16375 i 7679 Avg loss in epoch(incomplete): 0.159118535075\n",
      "Epoch: 1 global_step 16400 i 7704 Avg loss in epoch(incomplete): 0.159115255285\n",
      "Epoch: 1 global_step 16425 i 7729 Avg loss in epoch(incomplete): 0.15911457913\n",
      "Epoch: 1 global_step 16450 i 7754 Avg loss in epoch(incomplete): 0.1591121314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 global_step 16475 i 7779 Avg loss in epoch(incomplete): 0.159109641901\n",
      "saving\n",
      "Epoch: 1 global_step 16500 i 7804 Avg loss in epoch(incomplete): 0.159102954405\n",
      "Epoch: 1 global_step 16500 i 7804 Avg loss in epoch(incomplete): 0.159102954405\n",
      "Epoch: 1 global_step 16525 i 7829 Avg loss in epoch(incomplete): 0.159096842995\n",
      "Epoch: 1 global_step 16550 i 7854 Avg loss in epoch(incomplete): 0.15909424664\n",
      "Epoch: 1 global_step 16575 i 7879 Avg loss in epoch(incomplete): 0.15909086998\n",
      "Epoch: 1 global_step 16600 i 7904 Avg loss in epoch(incomplete): 0.159087350205\n",
      "Epoch: 1 global_step 16625 i 7929 Avg loss in epoch(incomplete): 0.159078573004\n",
      "Epoch: 1 global_step 16650 i 7954 Avg loss in epoch(incomplete): 0.159071750211\n",
      "Epoch: 1 global_step 16675 i 7979 Avg loss in epoch(incomplete): 0.159071607242\n",
      "Epoch: 1 global_step 16700 i 8004 Avg loss in epoch(incomplete): 0.159071731484\n",
      "Epoch: 1 global_step 16725 i 8029 Avg loss in epoch(incomplete): 0.159064410354\n",
      "saving\n",
      "Epoch: 1 global_step 16750 i 8054 Avg loss in epoch(incomplete): 0.159064430733\n",
      "Epoch: 1 global_step 16750 i 8054 Avg loss in epoch(incomplete): 0.159064430733\n",
      "Epoch: 1 global_step 16775 i 8079 Avg loss in epoch(incomplete): 0.159061835142\n",
      "Epoch: 1 global_step 16800 i 8104 Avg loss in epoch(incomplete): 0.159061670458\n",
      "Epoch: 1 global_step 16825 i 8129 Avg loss in epoch(incomplete): 0.159057463553\n",
      "Epoch: 1 global_step 16850 i 8154 Avg loss in epoch(incomplete): 0.159052943198\n",
      "Epoch: 1 global_step 16875 i 8179 Avg loss in epoch(incomplete): 0.159049981732\n",
      "Epoch: 1 global_step 16900 i 8204 Avg loss in epoch(incomplete): 0.159048853937\n",
      "Epoch: 1 global_step 16925 i 8229 Avg loss in epoch(incomplete): 0.159045503099\n",
      "Epoch: 1 global_step 16950 i 8254 Avg loss in epoch(incomplete): 0.159040738224\n",
      "Epoch: 1 global_step 16975 i 8279 Avg loss in epoch(incomplete): 0.159041132576\n",
      "saving\n",
      "Epoch: 1 global_step 17000 i 8304 Avg loss in epoch(incomplete): 0.159040423716\n",
      "Epoch: 1 global_step 17000 i 8304 Avg loss in epoch(incomplete): 0.159040423716\n",
      "Epoch: 1 global_step 17025 i 8329 Avg loss in epoch(incomplete): 0.159043733432\n",
      "Epoch: 1 global_step 17050 i 8354 Avg loss in epoch(incomplete): 0.159042713217\n",
      "Epoch: 1 global_step 17075 i 8379 Avg loss in epoch(incomplete): 0.159036364629\n",
      "Epoch: 1 global_step 17100 i 8404 Avg loss in epoch(incomplete): 0.159032332381\n",
      "Epoch: 1 global_step 17125 i 8429 Avg loss in epoch(incomplete): 0.159034092813\n",
      "Epoch: 1 global_step 17150 i 8454 Avg loss in epoch(incomplete): 0.159032421733\n",
      "Epoch: 1 global_step 17175 i 8479 Avg loss in epoch(incomplete): 0.159032273377\n",
      "Epoch: 1 global_step 17200 i 8504 Avg loss in epoch(incomplete): 0.159026150648\n",
      "Epoch: 1 global_step 17225 i 8529 Avg loss in epoch(incomplete): 0.159022773729\n",
      "saving\n",
      "Epoch: 1 global_step 17250 i 8554 Avg loss in epoch(incomplete): 0.159019234339\n",
      "Epoch: 1 global_step 17250 i 8554 Avg loss in epoch(incomplete): 0.159019234339\n",
      "Epoch: 1 global_step 17275 i 8579 Avg loss in epoch(incomplete): 0.15901889053\n",
      "Epoch: 1 global_step 17300 i 8604 Avg loss in epoch(incomplete): 0.159014816997\n",
      "Epoch: 1 global_step 17325 i 8629 Avg loss in epoch(incomplete): 0.159009118793\n",
      "Epoch: 1 global_step 17350 i 8654 Avg loss in epoch(incomplete): 0.159006075209\n",
      "Epoch: 1 global_step 17375 i 8679 Avg loss in epoch(incomplete): 0.159002599393\n",
      "saving\n",
      "Epoch: 1 global_step 17390 i 8694 Avg loss in epoch(incomplete): 0.158999943762\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists('log/' + modelname):\n",
    "   tf.gfile.DeleteRecursively('log/' + modelname) \n",
    "\n",
    "# train\n",
    "\n",
    "print_every = 25\n",
    "save_every = 250\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        global_step = 0\n",
    "    else:\n",
    "        saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load))\n",
    "        global_step = global_step_to_load\n",
    "    \n",
    "#    print(layer)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('log/' + modelname + '/train', sess.graph)\n",
    "    writerv = tf.summary.FileWriter('log/' + modelname + '/valid')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(num_batches_in_train):\n",
    "            batch_X, batch_Y = sess.run([seq_batch, label_batch])\n",
    "            batch_Xv, batch_Yv = sess.run([seqv_batch, labelv_batch])\n",
    "            _, _loss = sess.run([update, loss],\n",
    "                            feed_dict={X: batch_X, Y: batch_Y})\n",
    "            total_loss += _loss #/ num_batches_in_train\n",
    "            \n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                    Y: batch_Y})\n",
    "            summaryv = sess.run(summary_op, feed_dict={X: batch_Xv,\n",
    "                                                    Y: batch_Yv})\n",
    "#            writer.add_summary(summary, global_step=epoch*num_batches_in_train + i)\n",
    "#            writerv.add_summary(summaryv, global_step=epoch*num_batches_in_train + i)\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "            writerv.add_summary(summaryv, global_step=global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % save_every == 0:\n",
    "                print('saving')\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "                saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "                \n",
    "            if global_step % print_every == 0:\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "        \n",
    "        print('saving')\n",
    "        print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "\n",
    "        saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "    \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/dumbmotifconv-17390\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](shuffle_batch/random_shuffle_queue, Cast_1, Cast)]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# get test data\n",
    "\n",
    "global_step_to_load_test = 17390\n",
    "\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load_test))\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < 45458 / batch_size:\n",
    "        batch_Xt, batch_Yt = sess.run([seqt_batch, labelt_batch])\n",
    "        batch_logits = sess.run(logits, feed_dict={X: batch_Xt, Y: batch_Yt})\n",
    "        scores.append(batch_logits)\n",
    "        labels.append(batch_Yt)\n",
    "        #print('asdf')\n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "scores_arr = np.concatenate(scores, axis=0)\n",
    "labels_arr = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:\n",
      "[ 0.92887912  0.9883956   0.99098901  0.99452747  0.97518681  0.97395604\n",
      "  0.97516484  0.90767033  0.98136264  0.97830769  0.94145055  0.96775824\n",
      "  0.9496044   0.96784615  0.90465934  0.94035165  0.94389011  0.98320879\n",
      "  0.97936264  0.92076923  0.98982418  0.91635165  0.97138462  0.92907692]\n",
      "aucs:\n",
      "[0.57970390499110069, 0.84897469452373064, 0.67715209148099464, 0.51548830845247917, 0.69329574318195009, 0.69803365910810178, 0.50344500489647592, 0.66553373033632102, 0.72832572945664509, 0.69862531877405853, 0.60362325919625848, 0.71736309777334228, 0.53303165843762035, 0.67506511449523499, 0.7214642444771171, 0.720057221418407, 0.69399678454823266, 0.65305779947419274, 0.69433161932193843, 0.70509977473238616, 0.61081291883309197, 0.62713642089360055, 0.83174863894488482, 0.56698404825376525]\n",
      "auprcs:\n",
      "[0.090328430235127835, 0.045262361628946882, 0.01505762992213229, 0.0055397362376681065, 0.045095415626547827, 0.05001025825861543, 0.024803246126147472, 0.13505095708656828, 0.03918832382608075, 0.045494009729365792, 0.085815582038535815, 0.073718942185750411, 0.054162909250519822, 0.05399293634275, 0.19382266589110128, 0.11748092010139614, 0.091308591509142362, 0.026038034791798854, 0.040713880840099401, 0.1355854758760984, 0.014278893240964773, 0.12730762654485195, 0.10955995257402613, 0.086904047434377046]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>aucs</th>\n",
       "      <th>auprcs</th>\n",
       "      <th>props</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Pou2f2Pcr1x</th>\n",
       "      <td>0.975165</td>\n",
       "      <td>0.503445</td>\n",
       "      <td>0.024803</td>\n",
       "      <td>0.024835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Pax5c20V0416101</th>\n",
       "      <td>0.994527</td>\n",
       "      <td>0.515488</td>\n",
       "      <td>0.005540</td>\n",
       "      <td>0.005473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhGm12891NfkbTnfaIggrab</th>\n",
       "      <td>0.949604</td>\n",
       "      <td>0.533032</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.050396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UwHuvecCtcf</th>\n",
       "      <td>0.929077</td>\n",
       "      <td>0.566984</td>\n",
       "      <td>0.086904</td>\n",
       "      <td>0.070923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BroadHuvecCtcf</th>\n",
       "      <td>0.928879</td>\n",
       "      <td>0.579704</td>\n",
       "      <td>0.090328</td>\n",
       "      <td>0.071121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibHuvecPol24h8V0416101</th>\n",
       "      <td>0.941451</td>\n",
       "      <td>0.603623</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.058549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtaHuvecCmyc</th>\n",
       "      <td>0.989824</td>\n",
       "      <td>0.610813</td>\n",
       "      <td>0.014279</td>\n",
       "      <td>0.010176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtaHuvecCtcf</th>\n",
       "      <td>0.916352</td>\n",
       "      <td>0.627136</td>\n",
       "      <td>0.127308</td>\n",
       "      <td>0.083648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhHuvecMax</th>\n",
       "      <td>0.983209</td>\n",
       "      <td>0.653058</td>\n",
       "      <td>0.026038</td>\n",
       "      <td>0.016791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Pu1Pcr1x</th>\n",
       "      <td>0.907670</td>\n",
       "      <td>0.665534</td>\n",
       "      <td>0.135051</td>\n",
       "      <td>0.092330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhGm12891Pol2Iggmus</th>\n",
       "      <td>0.967846</td>\n",
       "      <td>0.675065</td>\n",
       "      <td>0.053993</td>\n",
       "      <td>0.032154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BroadHuvecPol2b</th>\n",
       "      <td>0.990989</td>\n",
       "      <td>0.677152</td>\n",
       "      <td>0.015058</td>\n",
       "      <td>0.009011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Pol24h8Pcr1x</th>\n",
       "      <td>0.975187</td>\n",
       "      <td>0.693296</td>\n",
       "      <td>0.045095</td>\n",
       "      <td>0.024813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhHuvecGata2Ucd</th>\n",
       "      <td>0.943890</td>\n",
       "      <td>0.693997</td>\n",
       "      <td>0.091309</td>\n",
       "      <td>0.056110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhHuvecPol2</th>\n",
       "      <td>0.979363</td>\n",
       "      <td>0.694332</td>\n",
       "      <td>0.040714</td>\n",
       "      <td>0.020637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Pol2Pcr1x</th>\n",
       "      <td>0.973956</td>\n",
       "      <td>0.698034</td>\n",
       "      <td>0.050010</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Yy1sc281V0416101</th>\n",
       "      <td>0.978308</td>\n",
       "      <td>0.698625</td>\n",
       "      <td>0.045494</td>\n",
       "      <td>0.021692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtaGm12891Ctcf</th>\n",
       "      <td>0.920769</td>\n",
       "      <td>0.705100</td>\n",
       "      <td>0.135585</td>\n",
       "      <td>0.079231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibHuvecPol2Pcr1x</th>\n",
       "      <td>0.967758</td>\n",
       "      <td>0.717363</td>\n",
       "      <td>0.073719</td>\n",
       "      <td>0.032242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhHuvecCjun</th>\n",
       "      <td>0.940352</td>\n",
       "      <td>0.720057</td>\n",
       "      <td>0.117481</td>\n",
       "      <td>0.059648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SydhHuvecCfosUcd</th>\n",
       "      <td>0.904659</td>\n",
       "      <td>0.721464</td>\n",
       "      <td>0.193823</td>\n",
       "      <td>0.095319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HaibGm12891Taf1Pcr1x</th>\n",
       "      <td>0.981363</td>\n",
       "      <td>0.728326</td>\n",
       "      <td>0.039188</td>\n",
       "      <td>0.018637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UtaHuvecPol2</th>\n",
       "      <td>0.971385</td>\n",
       "      <td>0.831749</td>\n",
       "      <td>0.109560</td>\n",
       "      <td>0.028615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BroadHuvecEzh239875</th>\n",
       "      <td>0.988396</td>\n",
       "      <td>0.848975</td>\n",
       "      <td>0.045262</td>\n",
       "      <td>0.011604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = np.mean((scores_arr > 0).astype(int) == labels_arr.astype(int), axis=0)\n",
    "\n",
    "print('accuracies:')\n",
    "print(accuracies)\n",
    "\n",
    "import sklearn.metrics\n",
    "print('aucs:')\n",
    "aucs = [sklearn.metrics.roc_auc_score(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(aucs)\n",
    "\n",
    "import sklearn.metrics\n",
    "def prec_recall(ys_true, ys_hat):\n",
    "    ys, xs, thresholds = sklearn.metrics.precision_recall_curve(ys_true, ys_hat)\n",
    "#     print(zip(xs,ys))\n",
    "    return sklearn.metrics.auc(xs, ys)\n",
    "\n",
    "print('auprcs:')\n",
    "auprcs = [prec_recall(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(auprcs)\n",
    "\n",
    "props = np.mean(labels_arr, axis=0)\n",
    "\n",
    "import json\n",
    "ids = sorted(json.loads(info['tf_to_pos'].replace(\"'\", '\"')).keys())\n",
    "\n",
    "from IPython.display import display_pretty, display_html\n",
    "import pandas as pd\n",
    "results = pd.DataFrame({'props':props, 'acc':accuracies, 'auprcs':auprcs, 'aucs':aucs}, index=ids).sort_values(by='aucs')\n",
    "display_html(results.to_html(), raw=True)\n",
    "results.to_csv('stats.' + modelname + '.tsv', index_label='id', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
