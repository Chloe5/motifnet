{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import fxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data locations\n",
    "raw_data_path = '../data/peaks_data'\n",
    "outbase = '../output/peaks_data'\n",
    "train_out_path = '%s/texttrain' % outbase\n",
    "valid_out_path = '%s/textvalidate' % outbase\n",
    "test_out_path = '%s/texttest' % outbase\n",
    "motif_path = '%s/../motifs' % raw_data_path\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 50\n",
    "num_batches_in_train = int(41305 / batch_size)\n",
    "num_epochs = 2\n",
    "capacity = 2000\n",
    "min_after_dequeue = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# retrieving models\n",
    "global_step_to_load = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs are of size 1200\n",
      "outputs are of size 12\n",
      "../output/peaks_data/texttest\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# get training data\n",
    "(seq, label), info = fxns.get_seq_and_label(train_out_path)\n",
    "seq_batch, label_batch = tf.train.shuffle_batch([seq, label],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('inputs are of size', info['seq_len'])\n",
    "\n",
    "# get validation data\n",
    "(seqv, labelv), infov = fxns.get_seq_and_label(valid_out_path)\n",
    "seqv_batch, labelv_batch = tf.train.shuffle_batch([seqv, labelv],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)\n",
    "print('outputs are of size', info['label_len'])\n",
    "\n",
    "reload(fxns)\n",
    "print(test_out_path)\n",
    "(seqt, labelt), info = fxns.get_seq_and_label(test_out_path, num_epochs=None)\n",
    "seqt_batch, labelt_batch = tf.train.shuffle_batch([seqt, labelt],\n",
    "    batch_size=batch_size,\n",
    "    capacity=capacity,\n",
    "    min_after_dequeue=min_after_dequeue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max filter length set to', 21)\n",
      "150.0\n",
      "75.0\n",
      "38.0\n",
      "1200 h 4 38.0\n"
     ]
    }
   ],
   "source": [
    "# # get model -- logistic\n",
    "# import logreg_model\n",
    "# modelname='logreg'\n",
    "# X, Y, loss, logits = logreg_model.get_logreg_model(info['seq_len'], info['label_len'])\n",
    "\n",
    "# get model -- generic convnet\n",
    "import motif_model; reload(motif_model)\n",
    "filters = motif_model.get_motifs(motif_path)\n",
    "modelname='peaks_motifconv'\n",
    "#conv_infos = [(7,(1,20),(2,2),4),(5,(1,20),(2,2),7)]#, (8,(1,20),(2,2),5)]\n",
    "conv_infos = [(14,(1,21),(2,2),2,4),(14,(1,8),(2,2),2,14),(7,(1,5),(2,2),2,14)]#, (8,(1,20),(2,2),5)]\n",
    "X, Y, loss, logits = motif_model.get_motif_model(info['seq_len'], info['label_len'], conv_infos, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate,\n",
    "                                                 name='SGD-Optimizer')\n",
    "    update = optimizer.minimize(loss)\n",
    "\n",
    "# define other summaries we want\n",
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.histogram('histogram-loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 global_step 25 i 24 Avg loss in epoch(incomplete): 0.6007275724411011\n",
      "Epoch: 0 global_step 50 i 49 Avg loss in epoch(incomplete): 0.5003329145908356\n",
      "Epoch: 0 global_step 75 i 74 Avg loss in epoch(incomplete): 0.4382620211442312\n",
      "Epoch: 0 global_step 100 i 99 Avg loss in epoch(incomplete): 0.4014534839987755\n",
      "Epoch: 0 global_step 125 i 124 Avg loss in epoch(incomplete): 0.3781718556880951\n",
      "Epoch: 0 global_step 150 i 149 Avg loss in epoch(incomplete): 0.36220197260379794\n",
      "Epoch: 0 global_step 175 i 174 Avg loss in epoch(incomplete): 0.35052662474768503\n",
      "Epoch: 0 global_step 200 i 199 Avg loss in epoch(incomplete): 0.34174059838056564\n",
      "Epoch: 0 global_step 225 i 224 Avg loss in epoch(incomplete): 0.33490647885534497\n",
      "saving\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.32939435839653014\n",
      "Epoch: 0 global_step 250 i 249 Avg loss in epoch(incomplete): 0.32939435839653014\n",
      "Epoch: 0 global_step 275 i 274 Avg loss in epoch(incomplete): 0.3248913269693201\n",
      "Epoch: 0 global_step 300 i 299 Avg loss in epoch(incomplete): 0.3211454493800799\n",
      "Epoch: 0 global_step 325 i 324 Avg loss in epoch(incomplete): 0.31795128868176387\n",
      "Epoch: 0 global_step 350 i 349 Avg loss in epoch(incomplete): 0.3152119645902089\n",
      "Epoch: 0 global_step 375 i 374 Avg loss in epoch(incomplete): 0.31286069401105243\n",
      "Epoch: 0 global_step 400 i 399 Avg loss in epoch(incomplete): 0.3107532924413681\n",
      "Epoch: 0 global_step 425 i 424 Avg loss in epoch(incomplete): 0.30888926351771634\n",
      "Epoch: 0 global_step 450 i 449 Avg loss in epoch(incomplete): 0.30728143162197535\n",
      "Epoch: 0 global_step 475 i 474 Avg loss in epoch(incomplete): 0.30582827097491216\n",
      "saving\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.3045552086830139\n",
      "Epoch: 0 global_step 500 i 499 Avg loss in epoch(incomplete): 0.3045552086830139\n",
      "Epoch: 0 global_step 525 i 524 Avg loss in epoch(incomplete): 0.30334025905245826\n",
      "Epoch: 0 global_step 550 i 549 Avg loss in epoch(incomplete): 0.30224309010939165\n",
      "Epoch: 0 global_step 575 i 574 Avg loss in epoch(incomplete): 0.3012874046097631\n",
      "Epoch: 0 global_step 600 i 599 Avg loss in epoch(incomplete): 0.3003578589856625\n",
      "Epoch: 0 global_step 625 i 624 Avg loss in epoch(incomplete): 0.2995400349140167\n",
      "Epoch: 0 global_step 650 i 649 Avg loss in epoch(incomplete): 0.298749733704787\n",
      "Epoch: 0 global_step 675 i 674 Avg loss in epoch(incomplete): 0.2980925824465575\n",
      "Epoch: 0 global_step 700 i 699 Avg loss in epoch(incomplete): 0.29743710190057754\n",
      "Epoch: 0 global_step 725 i 724 Avg loss in epoch(incomplete): 0.2968171256986158\n",
      "saving\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.29619741888840995\n",
      "Epoch: 0 global_step 750 i 749 Avg loss in epoch(incomplete): 0.29619741888840995\n",
      "Epoch: 0 global_step 775 i 774 Avg loss in epoch(incomplete): 0.2956327253003274\n",
      "Epoch: 0 global_step 800 i 799 Avg loss in epoch(incomplete): 0.29517929442226887\n",
      "Epoch: 0 global_step 825 i 824 Avg loss in epoch(incomplete): 0.2947078784306844\n",
      "saving\n",
      "Epoch: 0 global_step 826 i 825 Avg loss in epoch(incomplete): 0.2946886321029132\n",
      "Epoch: 1 global_step 850 i 23 Avg loss in epoch(incomplete): 0.2784039378166199\n",
      "Epoch: 1 global_step 875 i 48 Avg loss in epoch(incomplete): 0.27911191205589136\n",
      "Epoch: 1 global_step 900 i 73 Avg loss in epoch(incomplete): 0.278851185698767\n",
      "Epoch: 1 global_step 925 i 98 Avg loss in epoch(incomplete): 0.278949089122541\n",
      "Epoch: 1 global_step 950 i 123 Avg loss in epoch(incomplete): 0.2790486045902775\n",
      "Epoch: 1 global_step 975 i 148 Avg loss in epoch(incomplete): 0.27928017749882383\n",
      "saving\n",
      "Epoch: 1 global_step 1000 i 173 Avg loss in epoch(incomplete): 0.27911579934344893\n",
      "Epoch: 1 global_step 1000 i 173 Avg loss in epoch(incomplete): 0.27911579934344893\n",
      "Epoch: 1 global_step 1025 i 198 Avg loss in epoch(incomplete): 0.2791505990615442\n",
      "Epoch: 1 global_step 1050 i 223 Avg loss in epoch(incomplete): 0.27927130021687063\n",
      "Epoch: 1 global_step 1075 i 248 Avg loss in epoch(incomplete): 0.27905011739596786\n",
      "Epoch: 1 global_step 1100 i 273 Avg loss in epoch(incomplete): 0.27903423833586005\n",
      "Epoch: 1 global_step 1125 i 298 Avg loss in epoch(incomplete): 0.2791523337364197\n",
      "Epoch: 1 global_step 1150 i 323 Avg loss in epoch(incomplete): 0.27934021419949\n",
      "Epoch: 1 global_step 1175 i 348 Avg loss in epoch(incomplete): 0.2794564631914341\n",
      "Epoch: 1 global_step 1200 i 373 Avg loss in epoch(incomplete): 0.27945821744235444\n",
      "Epoch: 1 global_step 1225 i 398 Avg loss in epoch(incomplete): 0.27944851937449366\n",
      "saving\n",
      "Epoch: 1 global_step 1250 i 423 Avg loss in epoch(incomplete): 0.2795282924934378\n",
      "Epoch: 1 global_step 1250 i 423 Avg loss in epoch(incomplete): 0.2795282924934378\n",
      "Epoch: 1 global_step 1275 i 448 Avg loss in epoch(incomplete): 0.27956428174717657\n",
      "Epoch: 1 global_step 1300 i 473 Avg loss in epoch(incomplete): 0.2795793517229426\n",
      "Epoch: 1 global_step 1325 i 498 Avg loss in epoch(incomplete): 0.2795994388674925\n",
      "Epoch: 1 global_step 1350 i 523 Avg loss in epoch(incomplete): 0.2796238048272279\n",
      "Epoch: 1 global_step 1375 i 548 Avg loss in epoch(incomplete): 0.2796401340236212\n",
      "Epoch: 1 global_step 1400 i 573 Avg loss in epoch(incomplete): 0.27960930499880987\n",
      "Epoch: 1 global_step 1425 i 598 Avg loss in epoch(incomplete): 0.27957821728192106\n",
      "Epoch: 1 global_step 1450 i 623 Avg loss in epoch(incomplete): 0.2795542992938023\n",
      "Epoch: 1 global_step 1475 i 648 Avg loss in epoch(incomplete): 0.27956711277021284\n",
      "saving\n",
      "Epoch: 1 global_step 1500 i 673 Avg loss in epoch(incomplete): 0.2795124508064649\n",
      "Epoch: 1 global_step 1500 i 673 Avg loss in epoch(incomplete): 0.2795124508064649\n",
      "Epoch: 1 global_step 1525 i 698 Avg loss in epoch(incomplete): 0.27948374549547833\n",
      "Epoch: 1 global_step 1550 i 723 Avg loss in epoch(incomplete): 0.2795064928452613\n",
      "Epoch: 1 global_step 1575 i 748 Avg loss in epoch(incomplete): 0.2795254975278801\n",
      "Epoch: 1 global_step 1600 i 773 Avg loss in epoch(incomplete): 0.2795213442055138\n",
      "Epoch: 1 global_step 1625 i 798 Avg loss in epoch(incomplete): 0.2795029408269293\n",
      "Epoch: 1 global_step 1650 i 823 Avg loss in epoch(incomplete): 0.27953981492415214\n",
      "saving\n",
      "Epoch: 1 global_step 1652 i 825 Avg loss in epoch(incomplete): 0.2795349728974534\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists('log/' + modelname):\n",
    "   tf.gfile.DeleteRecursively('log/' + modelname) \n",
    "\n",
    "# train\n",
    "\n",
    "print_every = 25\n",
    "save_every = 250\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        global_step = 0\n",
    "    else:\n",
    "        saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load))\n",
    "        global_step = global_step_to_load\n",
    "    \n",
    "#    print(layer)\n",
    "    \n",
    "    writer = tf.summary.FileWriter('log/' + modelname + '/train', sess.graph)\n",
    "    writerv = tf.summary.FileWriter('log/' + modelname + '/valid')\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    if global_step_to_load is None:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(num_batches_in_train):\n",
    "            batch_X, batch_Y = sess.run([seq_batch, label_batch])\n",
    "            batch_Xv, batch_Yv = sess.run([seqv_batch, labelv_batch])\n",
    "            _, _loss = sess.run([update, loss],\n",
    "                            feed_dict={X: batch_X, Y: batch_Y})\n",
    "            total_loss += _loss #/ num_batches_in_train\n",
    "            \n",
    "            summary = sess.run(summary_op, feed_dict={X: batch_X,\n",
    "                                                    Y: batch_Y})\n",
    "            summaryv = sess.run(summary_op, feed_dict={X: batch_Xv,\n",
    "                                                    Y: batch_Yv})\n",
    "#            writer.add_summary(summary, global_step=epoch*num_batches_in_train + i)\n",
    "#            writerv.add_summary(summaryv, global_step=epoch*num_batches_in_train + i)\n",
    "            writer.add_summary(summary, global_step=global_step)\n",
    "            writerv.add_summary(summaryv, global_step=global_step)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % save_every == 0:\n",
    "                print('saving')\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "                saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "                \n",
    "            if global_step % print_every == 0:\n",
    "                print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "        \n",
    "        print('saving')\n",
    "        print('Epoch:',epoch,'global_step',global_step,'i',i,'Avg loss in epoch(incomplete):',total_loss / (i+1))\n",
    "\n",
    "        saver.save(sess, 'log/%s' % modelname, global_step=global_step)\n",
    "    \n",
    "    writer.close()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/peaks_motifconv-1652\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# get test data\n",
    "\n",
    "global_step_to_load_test = 1652\n",
    "\n",
    "scores = []\n",
    "labels = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=None)\n",
    "    saver.restore(sess, 'log/%s-%d' % (modelname, global_step_to_load_test))\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < 4348 / batch_size:\n",
    "        batch_Xt, batch_Yt = sess.run([seqt_batch, labelt_batch])\n",
    "        batch_logits = sess.run(logits, feed_dict={X: batch_Xt, Y: batch_Yt})\n",
    "        scores.append(batch_logits)\n",
    "        labels.append(batch_Yt)\n",
    "        #print('asdf')\n",
    "        idx += 1\n",
    "        if idx % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "scores_arr = np.concatenate(scores, axis=0)\n",
    "labels_arr = np.concatenate(labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:\n",
      "[0.89218391 0.90344828 0.89678161 0.90068966 0.91057471 0.91816092\n",
      " 0.92114943 0.98       0.90689655 0.8908046  0.97471264 0.9045977 ]\n",
      "aucs:\n",
      "[0.4286222474699056, 0.6471407367018054, 0.4532416735129877, 0.6345856847787041, 0.4080845440993128, 0.4977188394951978, 0.5442822000275029, 0.35120294649766365, 0.6110907696881503, 0.3603925297113752, 0.42243782161234994, 0.5170710797446456]\n",
      "auprcs:\n",
      "[0.09172802958020329, 0.13438808018222148, 0.09101745554824742, 0.14345659407416966, 0.07419584422826453, 0.08871942196419141, 0.09713572415214303, 0.021039345944296694, 0.12342984539682292, 0.08542903803703411, 0.02484537088919657, 0.10089759343274307]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>aucs</th>\n",
       "      <th>auprcs</th>\n",
       "      <th>props</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NFKB2</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.351203</td>\n",
       "      <td>0.021039</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPI1</th>\n",
       "      <td>0.890805</td>\n",
       "      <td>0.360393</td>\n",
       "      <td>0.085429</td>\n",
       "      <td>0.109195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAX</th>\n",
       "      <td>0.910575</td>\n",
       "      <td>0.408085</td>\n",
       "      <td>0.074196</td>\n",
       "      <td>0.089425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAF1</th>\n",
       "      <td>0.974713</td>\n",
       "      <td>0.422438</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>0.025287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CTCF</th>\n",
       "      <td>0.892184</td>\n",
       "      <td>0.428622</td>\n",
       "      <td>0.091728</td>\n",
       "      <td>0.107816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GATA2</th>\n",
       "      <td>0.896782</td>\n",
       "      <td>0.453242</td>\n",
       "      <td>0.091017</td>\n",
       "      <td>0.103218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MYC</th>\n",
       "      <td>0.918161</td>\n",
       "      <td>0.497719</td>\n",
       "      <td>0.088719</td>\n",
       "      <td>0.081839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBP</th>\n",
       "      <td>0.904598</td>\n",
       "      <td>0.517071</td>\n",
       "      <td>0.100898</td>\n",
       "      <td>0.095402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFKB1</th>\n",
       "      <td>0.921149</td>\n",
       "      <td>0.544282</td>\n",
       "      <td>0.097136</td>\n",
       "      <td>0.078851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAX5</th>\n",
       "      <td>0.906897</td>\n",
       "      <td>0.611091</td>\n",
       "      <td>0.123430</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JUN</th>\n",
       "      <td>0.900690</td>\n",
       "      <td>0.634586</td>\n",
       "      <td>0.143457</td>\n",
       "      <td>0.099310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOS</th>\n",
       "      <td>0.903448</td>\n",
       "      <td>0.647141</td>\n",
       "      <td>0.134388</td>\n",
       "      <td>0.096552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = np.mean((scores_arr > 0).astype(int) == labels_arr.astype(int), axis=0)\n",
    "\n",
    "print('accuracies:')\n",
    "print(accuracies)\n",
    "\n",
    "import sklearn.metrics\n",
    "print('aucs:')\n",
    "aucs = [sklearn.metrics.roc_auc_score(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(aucs)\n",
    "\n",
    "import sklearn.metrics\n",
    "def prec_recall(ys_true, ys_hat):\n",
    "    ys, xs, thresholds = sklearn.metrics.precision_recall_curve(ys_true, ys_hat)\n",
    "    return sklearn.metrics.auc(xs, ys, reorder=True)\n",
    "\n",
    "print('auprcs:')\n",
    "auprcs = [prec_recall(labels_arr[:,i],scores_arr[:,i]) for i in xrange(scores_arr.shape[1])]\n",
    "print(auprcs)\n",
    "\n",
    "props = np.mean(labels_arr, axis=0)\n",
    "\n",
    "import json\n",
    "ids = sorted(json.loads(info['tf_to_pos'].replace(\"'\", '\"')).keys())\n",
    "\n",
    "from IPython.display import display_pretty, display_html\n",
    "import pandas as pd\n",
    "results = pd.DataFrame({'props':props, 'acc':accuracies, 'auprcs':auprcs, 'aucs':aucs}, index=ids).sort_values(by='aucs')\n",
    "display_html(results.to_html(), raw=True)\n",
    "results.to_csv('stats.' + modelname + '.tsv', index_label='id', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
